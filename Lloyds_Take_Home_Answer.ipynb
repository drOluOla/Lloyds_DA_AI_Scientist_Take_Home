{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drOluOla/Lloyds_DA_AI_Scientist_Take_Home/blob/main/Lloyds_Take_Home_Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bfd5d6b"
      },
      "source": [
        "# **Introduction**\n",
        "Actionable insights into customer savings behavior are crucial for driving business growth and improving organisation's relationship with customer. The analysis in this report uses different Statistical and Machine Learning models to identify key patterns in historic saving behaviours for predicting future amounts that customers might save up. More importantly, the analysis identified multiple sensitive variables including protected characteristics (age, gender, ethnicity) and socioeconomic factors (income, credit score, savings). The most concerning pattern is age-based unfairness. A demo prototype showcasing how generative AI can be applied to produce personalized counterfactual savings analysis and actionable financial recommendations is also included in the report."
      ],
      "id": "0bfd5d6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a469fb01"
      },
      "source": [
        "# **Methods**\n",
        "\n",
        "Given that the dataset contained historic saving behaviour of customers that can be used for predicting future saving amounts. The prediction objective can be neatly mapped onto a standard regression task that requires multiple processing step. To manage my time, I broke the tasks into tickets and tracked progress iteratively on a Kanban board (image below), with subtasks detailed in the [GitHub project link](https://github.com/users/drOluOla/projects/2).[GitHub project link](https://github.com/users/drOluOla/projects/2).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=121BTN4Nhs75LCoV9K3zbkLwmYvOqDCNO)"
      ],
      "id": "a469fb01"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries and Tools Used:**\n",
        "-   **Stats Model**\n",
        "      - For Statistical Modelling\n",
        "-   **Scikit learn**\n",
        "      - For Machine Learning Modelling\n",
        "-   **Seaborn and Matplotlib **\n",
        "      - For visualisation\n",
        "-   **Fairlearn**\n",
        "      - Fairness analysis\n",
        "-   **Computer Assisted Coding (Gemini)**\n",
        "      - For code completion\n",
        "      - For code formatting\n",
        "      - For code documentation"
      ],
      "metadata": {
        "id": "8uGduj5d7elj"
      },
      "id": "8uGduj5d7elj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**"
      ],
      "metadata": {
        "id": "qmhFcLgW-ExG"
      },
      "id": "qmhFcLgW-ExG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Package Installation**"
      ],
      "metadata": {
        "id": "Xop3RWru-WWv"
      },
      "id": "Xop3RWru-WWv"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai-agents\n",
        "%pip install fairlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f79AikbIIHV",
        "outputId": "c19946d7-5c6e-4e58-9461-572ef067abc2"
      },
      "id": "5f79AikbIIHV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-agents in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: griffe<2,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.14.0)\n",
            "Requirement already satisfied: mcp<2,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.14.1)\n",
            "Requirement already satisfied: openai<2,>=1.107.1 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.108.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.11.9)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4)\n",
            "Requirement already satisfied: types-requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4.20250913)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (4.15.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe<2,>=1.5.6->openai-agents) (0.4.6)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.48.0)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.2.1)\n",
            "Requirement already satisfied: fairlearn in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.16.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from math import isfinite\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Union, Tuple, List, Any\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# Third-party core\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scientific/Statistical\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.discrete.discrete_model import Logit\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.stats.diagnostic import het_white\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "\n",
        "# Machine Learning - Sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_regression, mutual_info_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Machine Learning - Other\n",
        "import xgboost as xgb\n",
        "from fairlearn.metrics import MetricFrame\n",
        "\n",
        "# Custom modules\n",
        "from agents import Agent, Runner\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "4w7NO7ip-3p6"
      },
      "id": "4w7NO7ip-3p6",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Global Variables**"
      ],
      "metadata": {
        "id": "jpcUHm__FqDK"
      },
      "id": "jpcUHm__FqDK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths to the files in Google Drive\n",
        "data_path = \"/content/drive/My Drive/Job_2025/Lloyds/dataset/savings_customers_data_v1 8.xlsx\"\n",
        "dictionary_path = \"/content/drive/My Drive/Job_2025/Lloyds/dataset/data_dictionary_v1 8.xlsx\"\n",
        "\n",
        "data_exists = os.path.exists(data_path)\n",
        "dictionary_exists = os.path.exists(dictionary_path)\n",
        "\n",
        "# Verify paths to data and dictionary exist\n",
        "if data_exists and dictionary_exists:\n",
        "    print(\"Both data and dictionary files found!\")\n",
        "else:\n",
        "    if not data_exists:\n",
        "        print(f\"Error: Data file not found at {data_path}\")\n",
        "    if not dictionary_exists:\n",
        "        print(f\"Error: Dictionary file not found at {dictionary_path}\")\n",
        "    print(\"Please ensure both data and dictionary files are in the correct Google Drive location and the paths are accurate.\")\n",
        "\n",
        "\n",
        "# Verify API Key has been correctly setup for Question 6\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "instructions = \"You are a helpful econometric assistant. Reply in a 7-word clear and simple sentence\"\n",
        "agent = Agent(name=\"Assistant\", instructions=instructions)\n",
        "\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    result = await Runner.run(agent, \"How can Lloyds Bank ensure fairness and reduce bias using saving habits?\")\n",
        "    print(result.final_output)\n",
        "else:\n",
        "    print(\"Please set the OPENAI_API_KEY environment variable in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OdNagUhFt-R",
        "outputId": "c901b0ea-b376-40f8-c499-381c33092e0f"
      },
      "id": "7OdNagUhFt-R",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Both data and dictionary files found!\n",
            "Analyze diverse saving data, adjust practices accordingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utilities**"
      ],
      "metadata": {
        "id": "DS0JKebMz29X"
      },
      "id": "DS0JKebMz29X"
    },
    {
      "cell_type": "code",
      "source": [
        "class Utilities:\n",
        "    \"\"\"\n",
        "    A class for utility functions, including data visualization.\n",
        "    \"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, target_variable: str):\n",
        "        self.data = data\n",
        "        self.target_variable = target_variable\n",
        "\n",
        "    ####################################################\n",
        "    # Initial Exploration Helpers\n",
        "    ####################################################\n",
        "    def plot_target_distribution(self, output_dir: Path) -> None:\n",
        "        \"\"\"\n",
        "        Plots the distribution and box plot of the target variable.\n",
        "        \"\"\"\n",
        "        if self.target_variable in self.data.columns:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "            axes[0].hist(self.data[self.target_variable], bins=30, alpha=0.7)\n",
        "            axes[0].set_title('Target Distribution')\n",
        "            axes[1].boxplot(self.data[self.target_variable])\n",
        "            axes[1].set_title('Target Box Plot')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_dir / 'target_variable_distribution.png', dpi=200, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Target variable '{self.target_variable}' not found in the data.\")\n",
        "\n",
        "    def plot_correlation_matrix(self, output_dir: Path) -> None:\n",
        "        \"\"\"\n",
        "        Plots the correlation matrix of numeric variables.\n",
        "        \"\"\"\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        correlation_matrix = numeric_data.corr()\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0)\n",
        "        plt.title('Correlation Matrix')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / 'correlation_matrix.png', dpi=200, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_categorical_vs_target(self, output_dir: Path, num_cols_to_plot: int = 3) -> None:\n",
        "        \"\"\"\n",
        "        Plots box plots of the target variable against categorical variables.\n",
        "        \"\"\"\n",
        "        categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns\n",
        "        for col in categorical_columns[:num_cols_to_plot]:\n",
        "            if self.target_variable in self.data.columns:\n",
        "                plt.figure(figsize=(8, 5))\n",
        "                sns.boxplot(data=self.data, x=col, y=self.target_variable)\n",
        "                plt.title(f'{self.target_variable} by {col}')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(output_dir / f'{col}_vs_target.png', dpi=200, bbox_inches='tight')\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(f\"Target variable '{self.target_variable}' not found in the data.\")\n",
        "\n",
        "    def plot_all_distributions(self, output_dir: Path, num_categorical_cols: int = 3) -> None:\n",
        "        \"\"\"\n",
        "        Generates a composite figure with target distribution, correlation matrix,\n",
        "        and selected categorical vs. target variable plots.\n",
        "        \"\"\"\n",
        "        # Determine the number of rows needed: 1 for target, 1 for correlation,\n",
        "        # and ceil(num_categorical_cols / 2) for categorical plots (2 columns).\n",
        "        num_categorical_rows = (num_categorical_cols + 1) // 2\n",
        "        num_rows = 2 + num_categorical_rows\n",
        "        fig = plt.figure(figsize=(12, 4 * num_rows))\n",
        "        gs = fig.add_gridspec(num_rows, 2) # Always 2 columns\n",
        "\n",
        "        # Target Distribution\n",
        "        if self.target_variable in self.data.columns:\n",
        "            ax1 = fig.add_subplot(gs[0, 0])\n",
        "            ax1.hist(self.data[self.target_variable], bins=30, alpha=0.7)\n",
        "            ax1.set_title('Target Distribution')\n",
        "\n",
        "            ax2 = fig.add_subplot(gs[0, 1])\n",
        "            ax2.boxplot(self.data[self.target_variable])\n",
        "            ax2.set_title('Target Box Plot')\n",
        "        else:\n",
        "             print(f\"Target variable '{self.target_variable}' not found in the data.\")\n",
        "\n",
        "\n",
        "        # Correlation Matrix\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        correlation_matrix = numeric_data.corr()\n",
        "        ax3 = fig.add_subplot(gs[1, :]) # Span across both columns\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, ax=ax3)\n",
        "        ax3.set_title('Correlation Matrix')\n",
        "\n",
        "        # Categorical vs. Target Plots\n",
        "        categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns\n",
        "        for i, col in enumerate(categorical_columns[:num_categorical_cols]):\n",
        "            row_index = 2 + i // 2\n",
        "            col_index = i % 2\n",
        "            if self.target_variable in self.data.columns:\n",
        "                ax = fig.add_subplot(gs[row_index, col_index])\n",
        "                sns.boxplot(data=self.data, x=col, y=self.target_variable, ax=ax)\n",
        "                ax.set_title(f'{self.target_variable} by {col}')\n",
        "                plt.xticks(rotation=45)\n",
        "            else:\n",
        "                print(f\"Target variable '{self.target_variable}' not found in the data.\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / 'all_distributions_composite.png', dpi=200, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    ####################################################\n",
        "    # Data Cleaning and Processing Helpers\n",
        "    ####################################################\n",
        "    def show_data_types(self) -> None:\n",
        "        \"\"\"Displays the data types of each column.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DATA TYPES\")\n",
        "        print(\"=\"*50)\n",
        "        display(self.data.dtypes)\n",
        "\n",
        "    def show_missing_values_summary(self) -> None:\n",
        "        \"\"\"Display comprehensive missing values analysis.\"\"\"\n",
        "        missing_data_count = self.data.isnull().sum()\n",
        "        missing_data_percentage = (missing_data_count / len(self.data)) * 100\n",
        "        missing_data_df = pd.DataFrame({\n",
        "            'Count': missing_data_count,\n",
        "            'Percentage': missing_data_percentage\n",
        "        })\n",
        "        print(\"\\nMissing Values:\")\n",
        "        display(missing_data_df[missing_data_df['Count'] > 0])\n",
        "\n",
        "    def show_target_variable_analysis(self) -> None:\n",
        "        \"\"\"Comprehensive target variable analysis.\"\"\"\n",
        "        if self.target_variable not in self.data.columns:\n",
        "            print(f\"Target variable '{self.target_variable}' not found in data.\")\n",
        "            return\n",
        "\n",
        "        target = self.data[self.target_variable]\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"TARGET VARIABLE ANALYSIS: {self.target_variable}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Count: {len(target)}\")\n",
        "        print(f\"Mean: ${target.mean():,.2f}\")\n",
        "        print(f\"Median: ${target.median():,.2f}\")\n",
        "        print(f\"Std: ${target.std():,.2f}\")\n",
        "        print(f\"Min: ${target.min():,.2f}\")\n",
        "        print(f\"Max: ${target.max():,.2f}\")\n",
        "\n",
        "        # Zero-inflation check\n",
        "        zero_count = (target == 0).sum()\n",
        "        zero_pct = (zero_count / len(target)) * 100\n",
        "        print(f\"\\nZero values: {zero_count} ({zero_pct:.2f}%)\")\n",
        "        print(f\"Positive values: {(target > 0).sum()} ({100-zero_pct:.2f}%)\")\n",
        "\n",
        "        # Stats among savers only\n",
        "        positive_count = (target > 0).sum()\n",
        "        if positive_count > 0:\n",
        "            savers = target[target > 0]\n",
        "            print(f\"\\nAmong Savers Only (n={positive_count}):\")\n",
        "            print(f\"  Mean: ${savers.mean():,.2f}\")\n",
        "            print(f\"  Median: ${savers.median():,.2f}\")\n",
        "            print(f\"  Std Dev: ${savers.std():,.2f}\")\n",
        "            print(f\"  Min: ${savers.min():,.2f}\")\n",
        "            print(f\"  Max: ${savers.max():,.2f}\")\n",
        "\n",
        "    def check_zero_variance_columns(self) -> None:\n",
        "        \"\"\"Check for constant/zero-variance columns.\"\"\"\n",
        "        print(\"\\nChecking for zero-variance columns...\")\n",
        "        found_constant = False\n",
        "        for col in self.data.select_dtypes(include=[np.number]).columns:\n",
        "            if self.data[col].nunique() == 1:\n",
        "                print(f\"  ‚ö†Ô∏è  {col}: constant value ({self.data[col].iloc[0]})\")\n",
        "                found_constant = True\n",
        "        if not found_constant:\n",
        "            print(\"  ‚úì No zero-variance columns found\")\n",
        "\n",
        "    def validate_data_quality(self) -> None:\n",
        "        \"\"\"Comprehensive data quality validation checks.\"\"\"\n",
        "        print(\"\\nData Quality Validation:\")\n",
        "\n",
        "        # Check for remaining missing values\n",
        "        missing_total = self.data.isnull().sum().sum()\n",
        "        if missing_total > 0:\n",
        "            print(f\"  ‚ö†Ô∏è  WARNING: {missing_total} missing values remain\")\n",
        "            print(self.data.isnull().sum()[self.data.isnull().sum() > 0])\n",
        "        else:\n",
        "            print(f\"  ‚úì No missing values\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        inf_total = np.isinf(self.data[numeric_cols]).sum().sum()\n",
        "        if inf_total > 0:\n",
        "            print(f\"  ‚ö†Ô∏è  WARNING: {inf_total} infinite values remain\")\n",
        "        else:\n",
        "            print(f\"  ‚úì No infinite values\")\n",
        "\n",
        "        # Check target variable\n",
        "        if self.target_variable in self.data.columns:\n",
        "            target = self.data[self.target_variable]\n",
        "            negative_count = (target < 0).sum()\n",
        "            if negative_count > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  WARNING: {negative_count} negative values in target variable\")\n",
        "            else:\n",
        "                print(f\"  ‚úì Target variable is non-negative\")\n",
        "\n",
        "        # Check for duplicates\n",
        "        dup_count = self.data.duplicated().sum()\n",
        "        if dup_count > 0:\n",
        "            print(f\"  ‚ö†Ô∏è  WARNING: {dup_count} duplicate rows found\")\n",
        "        else:\n",
        "            print(f\"  ‚úì No duplicate rows\")\n",
        "\n",
        "    def show_basic_statistics(self) -> None:\n",
        "        \"\"\"Display basic statistical summary.\"\"\"\n",
        "        print(\"\\nBasic Statistics:\")\n",
        "        display(self.data.describe())\n",
        "\n",
        "    def plot_all_distributions(self, output_dir: Path) -> None:\n",
        "        \"\"\"\n",
        "        Plot distributions for all relevant variables.\n",
        "        (Keep your existing implementation here)\n",
        "        \"\"\"\n",
        "        # Your existing plotting code\n",
        "        pass\n",
        "\n",
        "    ####################################################\n",
        "    # Stats Models Helpers\n",
        "    ####################################################\n",
        "    @staticmethod\n",
        "    def prepare_model_data(data: pd.DataFrame, target_variable: str, max_categories: int = 50):\n",
        "        \"\"\"Prepare data for modeling by handling categoricals and cleaning.\"\"\"\n",
        "        model_data = data.copy()\n",
        "\n",
        "        # Remove ID columns\n",
        "        id_cols = [col for col in model_data.columns\n",
        "                   if 'id' in col.lower() or 'unique' in col.lower()]\n",
        "        if id_cols:\n",
        "            model_data = model_data.drop(columns=id_cols, errors='ignore')\n",
        "\n",
        "        # Convert string columns to numeric where possible\n",
        "        for col in model_data.columns:\n",
        "            if col != target_variable and model_data[col].dtype == 'object':\n",
        "                try:\n",
        "                    converted = pd.to_numeric(model_data[col], errors='coerce')\n",
        "                    if converted.notna().sum() / len(converted) > 0.8:\n",
        "                        model_data[col] = converted\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Handle categorical variables\n",
        "        categorical_cols = model_data.select_dtypes(include=['object', 'category']).columns\n",
        "        categorical_cols = [col for col in categorical_cols if col != target_variable]\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            n_unique = model_data[col].nunique()\n",
        "            if n_unique > max_categories:\n",
        "                top_cats = model_data[col].value_counts().nlargest(max_categories).index\n",
        "                model_data[col] = model_data[col].where(\n",
        "                    model_data[col].isin(top_cats), other='_other_'\n",
        "                )\n",
        "            dummies = pd.get_dummies(model_data[col], prefix=col, drop_first=True, dtype=int)\n",
        "            model_data = pd.concat([model_data, dummies], axis=1)\n",
        "            model_data.drop(col, axis=1, inplace=True)\n",
        "\n",
        "        # Split features and target\n",
        "        y = model_data[target_variable]\n",
        "        X = model_data.drop(target_variable, axis=1).select_dtypes(include=[np.number])\n",
        "\n",
        "        # Clean data\n",
        "        combined = pd.concat([X, y], axis=1)\n",
        "        combined = combined.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(combined) == 0:\n",
        "            return None, None, None, None\n",
        "\n",
        "        X_clean = combined.iloc[:, :-1]\n",
        "        y_clean = combined.iloc[:, -1]\n",
        "        is_saver = (y_clean > 0).astype(int)\n",
        "\n",
        "        return X_clean, y_clean, is_saver, X\n",
        "\n",
        "    @staticmethod\n",
        "    def select_features(X: pd.DataFrame, y: pd.Series, method: str = 'mutual_info',\n",
        "                       max_features: int = 30, task: str = 'regression',\n",
        "                       filter_high_correlation: bool = True, correlation_threshold: float = 0.70):\n",
        "        \"\"\"\n",
        "        Generic feature selection for both classification and regression tasks.\n",
        "        \"\"\"\n",
        "        # Filter high correlations if requested\n",
        "        if filter_high_correlation:\n",
        "            correlations = X.corrwith(y).abs()\n",
        "            high_corr_features = correlations[correlations > correlation_threshold].index.tolist()\n",
        "            if high_corr_features:\n",
        "                X_filtered = X.drop(columns=high_corr_features)\n",
        "            else:\n",
        "                X_filtered = X.copy()\n",
        "        else:\n",
        "            X_filtered = X.copy()\n",
        "\n",
        "        if X_filtered.shape[1] == 0:\n",
        "            return sm.add_constant(pd.DataFrame()), []\n",
        "\n",
        "        # Select features based on method and task\n",
        "        if task == 'classification':\n",
        "            if method == 'mutual_info':\n",
        "                mi_scores = mutual_info_classif(X_filtered, y, random_state=42)\n",
        "                feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "            elif method == 'f_test':\n",
        "                selector = SelectKBest(score_func=f_classif, k='all')\n",
        "                selector.fit(X_filtered, y)\n",
        "                feature_scores = pd.Series(selector.scores_, index=X_filtered.columns)\n",
        "            elif method == 'correlation':\n",
        "                feature_scores = X_filtered.corrwith(y).abs()\n",
        "            else:\n",
        "                mi_scores = mutual_info_classif(X_filtered, y, random_state=42)\n",
        "                feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "        else:  # regression\n",
        "            if method == 'mutual_info':\n",
        "                mi_scores = mutual_info_regression(X_filtered, y, random_state=42)\n",
        "                feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "            elif method == 'f_test':\n",
        "                selector = SelectKBest(score_func=f_regression, k='all')\n",
        "                selector.fit(X_filtered, y)\n",
        "                feature_scores = pd.Series(selector.scores_, index=X_filtered.columns)\n",
        "            elif method == 'lasso':\n",
        "                lasso = LassoCV(cv=5, random_state=42, max_iter=1000)\n",
        "                lasso.fit(X_filtered, y)\n",
        "                feature_scores = pd.Series(np.abs(lasso.coef_), index=X_filtered.columns)\n",
        "            elif method == 'correlation':\n",
        "                feature_scores = X_filtered.corrwith(y).abs()\n",
        "            else:\n",
        "                mi_scores = mutual_info_regression(X_filtered, y, random_state=42)\n",
        "                feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "\n",
        "        top_features = feature_scores.nlargest(max_features).index.tolist()\n",
        "        X_selected = X_filtered[top_features]\n",
        "\n",
        "        return sm.add_constant(X_selected), top_features\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        \"\"\"Calculate common regression metrics.\"\"\"\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        baseline_pred = np.full(len(y_true), y_true.mean())\n",
        "        baseline_rmse = np.sqrt(mean_squared_error(y_true, baseline_pred))\n",
        "        improvement = ((baseline_rmse - rmse) / baseline_rmse) * 100\n",
        "\n",
        "        return {\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2,\n",
        "            'baseline_rmse': baseline_rmse,\n",
        "            'improvement_pct': improvement\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_diagnostics(y_clean, results, output_dir: Path):\n",
        "        \"\"\"Generate diagnostic plots for model evaluation.\"\"\"\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
        "            fig.suptitle('Model Diagnostics', fontsize=16, fontweight='bold')\n",
        "\n",
        "            # Two-Part: Actual vs Predicted\n",
        "            if 'final_predictions' in results:\n",
        "                ax = axes[0, 0]\n",
        "                ax.scatter(y_clean, results['final_predictions'],\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                max_val = max(y_clean.max(), results['final_predictions'].max())\n",
        "                ax.plot([0, max_val], [0, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "                ax.set_xlabel('Actual Savings (¬£)')\n",
        "                ax.set_ylabel('Predicted Savings (¬£)')\n",
        "                ax.set_title('Two-Part Model: Actual vs Predicted')\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Two-Part: Residual Plot\n",
        "            if 'final_predictions' in results:\n",
        "                ax = axes[0, 1]\n",
        "                residuals = y_clean - results['final_predictions']\n",
        "                ax.scatter(results['final_predictions'], residuals,\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "                ax.set_xlabel('Predicted Savings (¬£)')\n",
        "                ax.set_ylabel('Residuals (¬£)')\n",
        "                ax.set_title('Two-Part Model: Residual Plot')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Distribution Comparison\n",
        "            ax = axes[1, 0]\n",
        "            ax.hist(y_clean, bins=50, alpha=0.6, label='Actual', edgecolor='black')\n",
        "            if 'final_predictions' in results:\n",
        "                ax.hist(results['final_predictions'], bins=50, alpha=0.6,\n",
        "                       label='Two-Part', edgecolor='black')\n",
        "            if 'ols_only_predictions' in results:\n",
        "                ax.hist(results['ols_only_predictions'], bins=50, alpha=0.6,\n",
        "                       label='OLS-Only', edgecolor='black')\n",
        "            ax.set_xlabel('Savings (¬£)')\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.set_title('Distribution Comparison')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "            # R¬≤ Comparison\n",
        "            if 'comparison_table' in results:\n",
        "                ax = axes[1, 1]\n",
        "                comp_df = results['comparison_table']\n",
        "                models = comp_df['Model'].tolist()\n",
        "                r2_values = comp_df['R¬≤'].tolist()\n",
        "\n",
        "                bars = ax.barh(models, r2_values, color=['lightgray', 'skyblue', 'lightgreen'])\n",
        "                ax.set_xlabel('R¬≤ Score')\n",
        "                ax.set_title('Model Performance (R¬≤)')\n",
        "                ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "                for i, (bar, val) in enumerate(zip(bars, r2_values)):\n",
        "                    ax.text(val + 0.01, i, f'{val:.4f}', va='center')\n",
        "\n",
        "            # OLS-Only diagnostics\n",
        "            if 'ols_only_predictions' in results and 'ols_only_model' in results:\n",
        "                ols_only_pred = results['ols_only_predictions']\n",
        "                ols_only_residuals = y_clean - ols_only_pred\n",
        "\n",
        "                ax = axes[2, 0]\n",
        "                ax.scatter(ols_only_pred, ols_only_residuals,\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "                ax.set_xlabel('Fitted Values (¬£)')\n",
        "                ax.set_ylabel('Residuals (¬£)')\n",
        "                ax.set_title('OLS-Only: Residuals vs Fitted')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                ax = axes[2, 1]\n",
        "                standardized_residuals = ols_only_residuals / ols_only_residuals.std()\n",
        "                stats.probplot(standardized_residuals, dist=\"norm\", plot=ax)\n",
        "                ax.set_title('OLS-Only: Q-Q Plot')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plot_path = output_dir / 'model_diagnostics.png'\n",
        "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"\\nüíæ Saved: {plot_path}\")\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Could not generate plots: {e}\")\n",
        "\n",
        "    ####################################################\n",
        "    # ML Models Helpers\n",
        "    ####################################################\n",
        "    @staticmethod\n",
        "    def remove_id_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Remove ID and unique identifier columns from dataframe.\"\"\"\n",
        "        id_cols = [col for col in df.columns\n",
        "                   if 'id' in col.lower() or 'unique' in col.lower()]\n",
        "        if id_cols:\n",
        "            df = df.drop(columns=id_cols, errors='ignore')\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_numeric_strings(df: pd.DataFrame, target_variable: str, threshold: float = 0.8) -> pd.DataFrame:\n",
        "        \"\"\"Convert string columns to numeric if >threshold% of values are numeric.\"\"\"\n",
        "        for col in df.columns:\n",
        "            if col != target_variable and df[col].dtype == 'object':\n",
        "                try:\n",
        "                    converted = pd.to_numeric(df[col], errors='coerce')\n",
        "                    if converted.notna().sum() / len(converted) > threshold:\n",
        "                        df[col] = converted\n",
        "                except:\n",
        "                    pass\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_categorical_features(df: pd.DataFrame, target_variable: str, max_categories: int = 50) -> pd.DataFrame:\n",
        "        \"\"\"One-hot encode categorical features with category limiting.\"\"\"\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "        categorical_cols = [col for col in categorical_cols if col != target_variable]\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            n_unique = df[col].nunique()\n",
        "            if n_unique > max_categories:\n",
        "                top_cats = df[col].value_counts().nlargest(max_categories).index\n",
        "                df[col] = df[col].where(df[col].isin(top_cats), other='_other_')\n",
        "\n",
        "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True, dtype=int)\n",
        "            df = pd.concat([df, dummies], axis=1)\n",
        "            df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_infinite_and_nan(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"Remove infinite values and NaN from features and target.\"\"\"\n",
        "        combined = pd.concat([X, y], axis=1)\n",
        "        combined = combined.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(combined) == 0:\n",
        "            return pd.DataFrame(), pd.Series()\n",
        "\n",
        "        X_clean = combined.iloc[:, :-1]\n",
        "        y_clean = combined.iloc[:, -1]\n",
        "        X_clean = X_clean.loc[y_clean.index]\n",
        "\n",
        "        return X_clean, y_clean\n",
        "\n",
        "    @staticmethod\n",
        "    def stratified_train_test_split(X: pd.DataFrame, y: pd.Series,\n",
        "                                   test_size: float = 0.2,\n",
        "                                   random_state: int = 42) -> Tuple:\n",
        "        \"\"\"Perform stratified train-test split with fallback for edge cases.\"\"\"\n",
        "        try:\n",
        "            if y.nunique() > 1:\n",
        "                y_quantiles = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
        "                return train_test_split(X, y, test_size=test_size,\n",
        "                                      random_state=random_state,\n",
        "                                      stratify=y_quantiles)\n",
        "            else:\n",
        "                print(\"Warning: Target variable has only one unique value. Splitting without stratification.\")\n",
        "                return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not stratify on y quantiles ({e}). Splitting without stratification.\")\n",
        "            return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_dataframe(df: pd.DataFrame, filename: str, output_dir: str = \"analysis_outputs\") -> None:\n",
        "        \"\"\"Save dataframe to CSV in specified directory.\"\"\"\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(exist_ok=True)\n",
        "        filepath = output_path / filename\n",
        "        df.to_csv(filepath, index=False)\n",
        "        print(f\"\\nüíæ Saved: {filepath}\")\n",
        "\n",
        "    ####################################################\n",
        "    # Ethic Analysis Helpers\n",
        "    ####################################################\n"
      ],
      "metadata": {
        "id": "caihYQ1Cz9y1"
      },
      "id": "caihYQ1Cz9y1",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1: Clean Dataset & Conduct Descriptive Analysis**\n",
        "Sub-tasks\n",
        "\n",
        "- Load, clean and perform initial exploration\n",
        "- Perform data transformations and feature engineering as needed\n",
        "- Generate descriptive statistics for all variables\n",
        "- Create visualizations (plots) to explore variable distributions and relationships\n",
        "\n"
      ],
      "metadata": {
        "id": "YKMVAijd_KRh"
      },
      "id": "YKMVAijd_KRh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "5GuMVtWXBEeA"
      },
      "id": "5GuMVtWXBEeA"
    },
    {
      "cell_type": "code",
      "source": [
        "class LBGDataLoader:\n",
        "    \"\"\"\n",
        "    A class to load customer savings data and its dictionary.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path: str, dictionary_path: str):\n",
        "        self.data_path: Path = Path(data_path)\n",
        "        self.dictionary_path: Path = Path(dictionary_path)\n",
        "        self.raw_data: Optional[pd.DataFrame] = None\n",
        "        self.data_dictionary: Optional[pd.DataFrame] = None\n",
        "        self.target_variable = 'annual_net_savings_lbg'\n",
        "        #self.results = {}\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads the data and data dictionary from the specified paths.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.raw_data = pd.read_excel(self.data_path)\n",
        "            self.data_dictionary = pd.read_excel(self.dictionary_path)\n",
        "            print(f\"‚úì Loaded {len(self.raw_data)} records with {len(self.raw_data.columns)} columns\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "LHd_l2ok_0SV"
      },
      "id": "LHd_l2ok_0SV",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LBGDataProcessor(LBGDataLoader):\n",
        "    \"\"\"\n",
        "    A class to process the customer savings data with proper handling for two-part modeling.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path: str, dictionary_path: str):\n",
        "        super().__init__(data_path, dictionary_path)\n",
        "        self.clean_data: Optional[pd.DataFrame] = None\n",
        "        self.preprocessing_log = {}\n",
        "\n",
        "    def initial_data_exploration(self) -> None:\n",
        "        \"\"\"Comprehensive initial exploration\"\"\"\n",
        "        print(\"=\"*50, \"DATA EXPLORATION\", \"=\"*50)\n",
        "        print(f\"Shape: {self.raw_data.shape}\")\n",
        "        print(f\"Columns: {list(self.raw_data.columns)}\")\n",
        "\n",
        "        # Use Utilities class for analysis\n",
        "        utilities = Utilities(self.raw_data, self.target_variable)\n",
        "        utilities.show_missing_values_summary()\n",
        "        utilities.show_target_variable_analysis()\n",
        "        utilities.show_data_types()\n",
        "        utilities.show_basic_statistics()\n",
        "        utilities.check_zero_variance_columns()\n",
        "\n",
        "        print(f\"\\nDuplicates: {self.raw_data.duplicated().sum()}\")\n",
        "\n",
        "    def clean_and_preprocess_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Clean and preprocess data with proper handling for statistical modeling.\n",
        "        CRITICAL: Does NOT remove outliers for two-part modeling.\n",
        "        \"\"\"\n",
        "        print(\"=\"*50, \"DATA CLEANING\", \"=\"*50)\n",
        "        self.clean_data = self.raw_data.copy()\n",
        "\n",
        "        initial_shape = self.clean_data.shape\n",
        "        self.preprocessing_log['initial_shape'] = initial_shape\n",
        "\n",
        "        # Step 2: Handle suspicious ages\n",
        "        self._handle_age_issues()\n",
        "\n",
        "        # Step 3: Handle missing values strategically\n",
        "        self._handle_missing_values_strategic()\n",
        "\n",
        "        # Step 4: Create indicator variables for missingness\n",
        "        self._create_missing_indicators()\n",
        "\n",
        "        # Step 5: Remove duplicates\n",
        "        duplicates_removed = self.clean_data.duplicated().sum()\n",
        "        self.clean_data.drop_duplicates(inplace=True)\n",
        "        self.preprocessing_log['duplicates_removed'] = duplicates_removed\n",
        "\n",
        "        # Step 6: Handle infinite values\n",
        "        self._handle_infinite_values()\n",
        "\n",
        "        # Step 7: Engineer features AFTER cleaning\n",
        "        self._engineer_features()\n",
        "\n",
        "        # Step 8: Final validation using Utilities\n",
        "        utilities = Utilities(self.clean_data, self.target_variable)\n",
        "        utilities.validate_data_quality()\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(\"PREPROCESSING SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Initial shape: {initial_shape}\")\n",
        "        print(f\"Final shape: {self.clean_data.shape}\")\n",
        "        print(f\"Rows removed: {initial_shape[0] - self.clean_data.shape[0]}\")\n",
        "        print(f\"Duplicates removed: {duplicates_removed}\")\n",
        "        print(f\"Missing values remaining: {self.clean_data.isnull().sum().sum()}\")\n",
        "\n",
        "    def _handle_age_issues(self) -> None:\n",
        "        \"\"\"Handle suspicious age values\"\"\"\n",
        "        if 'age' not in self.clean_data.columns:\n",
        "            return\n",
        "\n",
        "        suspicious_ages = self.clean_data[self.clean_data['age'] < 18]\n",
        "        if len(suspicious_ages) > 0:\n",
        "            print(f\"\\n‚ö†Ô∏è  Found {len(suspicious_ages)} records with age < 18\")\n",
        "            print(f\"   Age range: {suspicious_ages['age'].min()} to {suspicious_ages['age'].max()}\")\n",
        "            print(f\"   Removing these records (likely data errors)...\")\n",
        "\n",
        "            self.clean_data = self.clean_data[self.clean_data['age'] >= 18].copy()\n",
        "            self.preprocessing_log['suspicious_ages_removed'] = len(suspicious_ages)\n",
        "\n",
        "    def _handle_missing_values_strategic(self) -> None:\n",
        "        \"\"\"Handle missing values based on business logic and statistical requirements.\"\"\"\n",
        "        print(\"\\nHandling missing values strategically:\")\n",
        "\n",
        "        # 1. other_income: Missing likely means zero\n",
        "        if 'other_income' in self.clean_data.columns:\n",
        "            missing_count = self.clean_data['other_income'].isnull().sum()\n",
        "            if missing_count > 0:\n",
        "                print(f\"  ‚úì other_income: Imputing {missing_count} missing ‚Üí 0 (no other income)\")\n",
        "                self.clean_data['other_income'].fillna(0, inplace=True)\n",
        "\n",
        "        # 2. housing_spend: Missing is informative, handle with indicator + imputation\n",
        "        if 'housing_spend' in self.clean_data.columns:\n",
        "            missing_count = self.clean_data['housing_spend'].isnull().sum()\n",
        "            if missing_count > 0:\n",
        "                print(f\"  ‚úì housing_spend: {missing_count} missing values detected\")\n",
        "                print(f\"     ‚Üí Creating 'has_housing_spend' indicator\")\n",
        "                print(f\"     ‚Üí Imputing with region-specific median\")\n",
        "\n",
        "                # Group-based imputation\n",
        "                if 'geo_region' in self.clean_data.columns:\n",
        "                    self.clean_data['housing_spend'] = self.clean_data.groupby('geo_region')['housing_spend'].transform(\n",
        "                        lambda x: x.fillna(x.median())\n",
        "                    )\n",
        "                else:\n",
        "                    self.clean_data['housing_spend'].fillna(\n",
        "                        self.clean_data['housing_spend'].median(), inplace=True\n",
        "                    )\n",
        "\n",
        "        # 3. Handle other numeric columns (< 5% missing ‚Üí median imputation)\n",
        "        numeric_columns = self.clean_data.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            if col in ['other_income', 'housing_spend']:\n",
        "                continue\n",
        "\n",
        "            missing_count = self.clean_data[col].isnull().sum()\n",
        "            missing_pct = (missing_count / len(self.clean_data)) * 100\n",
        "\n",
        "            if 0 < missing_pct < 5:\n",
        "                print(f\"  ‚úì {col}: Imputing {missing_count} missing ({missing_pct:.2f}%) ‚Üí median\")\n",
        "                self.clean_data[col].fillna(self.clean_data[col].median(), inplace=True)\n",
        "            elif missing_pct >= 5:\n",
        "                print(f\"  ‚ö†Ô∏è  {col}: {missing_pct:.2f}% missing - consider dropping or advanced imputation\")\n",
        "\n",
        "        # 4. Categorical columns: mode imputation\n",
        "        categorical_columns = self.clean_data.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            missing_count = self.clean_data[col].isnull().sum()\n",
        "            missing_pct = (missing_count / len(self.clean_data)) * 100\n",
        "\n",
        "            if 0 < missing_pct < 5:\n",
        "                mode_value = self.clean_data[col].mode()[0] if not self.clean_data[col].mode().empty else 'Unknown'\n",
        "                print(f\"  ‚úì {col}: Imputing {missing_count} missing ({missing_pct:.2f}%) ‚Üí mode ({mode_value})\")\n",
        "                self.clean_data[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "    def _create_missing_indicators(self) -> None:\n",
        "        \"\"\"Create binary indicators for originally missing values\"\"\"\n",
        "        print(\"\\nCreating missingness indicators:\")\n",
        "\n",
        "        # housing_spend indicator\n",
        "        if 'housing_spend' in self.raw_data.columns:\n",
        "            self.clean_data['has_housing_spend'] = self.raw_data['housing_spend'].notna().astype(int)\n",
        "            print(f\"  ‚úì Created 'has_housing_spend' indicator\")\n",
        "            print(f\"     ‚Üí {self.clean_data['has_housing_spend'].sum()} with housing spend\")\n",
        "            print(f\"     ‚Üí {(~self.clean_data['has_housing_spend'].astype(bool)).sum()} without\")\n",
        "\n",
        "    def _handle_infinite_values(self) -> None:\n",
        "        \"\"\"Replace infinite values with NaN, then impute\"\"\"\n",
        "        numeric_columns = self.clean_data.select_dtypes(include=[np.number]).columns\n",
        "        inf_count = 0\n",
        "\n",
        "        for col in numeric_columns:\n",
        "            inf_mask = np.isinf(self.clean_data[col])\n",
        "            col_inf_count = inf_mask.sum()\n",
        "\n",
        "            if col_inf_count > 0:\n",
        "                inf_count += col_inf_count\n",
        "                # Replace inf with nan\n",
        "                self.clean_data.loc[inf_mask, col] = np.nan\n",
        "                # Impute with median\n",
        "                median_value = self.clean_data[col].median()\n",
        "                self.clean_data[col].fillna(median_value, inplace=True)\n",
        "\n",
        "        if inf_count > 0:\n",
        "            print(f\"\\n  ‚úì Replaced {inf_count} infinite values with median\")\n",
        "\n",
        "    def _engineer_features(self) -> None:\n",
        "        \"\"\"Engineer features AFTER cleaning, with proper validation.\"\"\"\n",
        "        print(\"\\nEngineering features:\")\n",
        "        features_added = []\n",
        "\n",
        "        # 1. Savings rate (if we have income data)\n",
        "        if 'observed_income' in self.clean_data.columns and self.target_variable in self.clean_data.columns:\n",
        "            # Avoid division by zero\n",
        "            self.clean_data['savings_rate'] = np.where(\n",
        "                self.clean_data['observed_income'] > 0,\n",
        "                self.clean_data[self.target_variable] / self.clean_data['observed_income'],\n",
        "                0\n",
        "            )\n",
        "            # Cap at reasonable bounds (0-100%)\n",
        "            self.clean_data['savings_rate'] = np.clip(self.clean_data['savings_rate'], 0, 1)\n",
        "            features_added.append('savings_rate')\n",
        "\n",
        "        # 2. Age groups\n",
        "        if 'age' in self.clean_data.columns:\n",
        "            self.clean_data['age_group'] = pd.cut(\n",
        "                self.clean_data['age'],\n",
        "                bins=[18, 30, 40, 50, 60, 100],\n",
        "                labels=['18-29', '30-39', '40-49', '50-59', '60+'],\n",
        "                include_lowest=True\n",
        "            )\n",
        "            features_added.append('age_group')\n",
        "\n",
        "        # 3. Income-to-spend ratio (discretionary income indicator)\n",
        "        if all(col in self.clean_data.columns for col in ['observed_income', 'housing_spend', 'childcare_spend']):\n",
        "            total_fixed_spend = self.clean_data['housing_spend'] + self.clean_data['childcare_spend']\n",
        "            self.clean_data['discretionary_income'] = self.clean_data['observed_income'] - total_fixed_spend\n",
        "            self.clean_data['discretionary_income'] = np.maximum(self.clean_data['discretionary_income'], 0)\n",
        "            features_added.append('discretionary_income')\n",
        "\n",
        "        # 4. High saver indicator (useful for segmentation)\n",
        "        if self.target_variable in self.clean_data.columns:\n",
        "            median_savings = self.clean_data[self.clean_data[self.target_variable] > 0][self.target_variable].median()\n",
        "            self.clean_data['is_high_saver'] = (self.clean_data[self.target_variable] > median_savings).astype(int)\n",
        "            features_added.append('is_high_saver')\n",
        "\n",
        "        print(f\"  ‚úì Added {len(features_added)} features: {features_added}\")\n",
        "        self.preprocessing_log['features_engineered'] = features_added\n",
        "\n",
        "    def descriptive_analysis(self) -> None:\n",
        "        \"\"\"Enhanced descriptive analysis for two-part modeling\"\"\"\n",
        "        print(\"=\"*50, \"DESCRIPTIVE ANALYSIS\", \"=\"*50)\n",
        "        output_dir = Path(\"analysis_outputs\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Use Utilities class for analysis and visualization\n",
        "        utilities = Utilities(self.clean_data, self.target_variable)\n",
        "        utilities.show_basic_statistics()\n",
        "        utilities.show_target_variable_analysis()\n",
        "        utilities.plot_all_distributions(output_dir)\n",
        "\n",
        "        print(f\"\\n‚úì Analysis outputs saved to '{output_dir}/'\")"
      ],
      "metadata": {
        "id": "noaJw0btjfsY"
      },
      "execution_count": 17,
      "outputs": [],
      "id": "noaJw0btjfsY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8fea843e",
        "outputId": "71bd6897-c598-4559-8a67-c314c0cf8680"
      },
      "source": [
        "#run the codes above to test\n",
        "data_processor = LBGDataProcessor(data_path, dictionary_path)\n",
        "data_processor.load_data()\n",
        "data_processor.initial_data_exploration()\n",
        "data_processor.clean_and_preprocess_data()\n",
        "data_processor.descriptive_analysis()"
      ],
      "id": "8fea843e",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Loaded 30305 records with 15 columns\n",
            "================================================== DATA EXPLORATION ==================================================\n",
            "Shape: (30305, 15)\n",
            "Columns: ['cust_unique_id', 'age', 'gender', 'ethnicity_group', 'geo_region', 'net_salary', 'other_income', 'observed_income', 'housing_spend', 'childcare_spend', 'gambling_spend', 'observed_surplus', 'credit_score', 'savings_bal_lbg', 'annual_net_savings_lbg']\n",
            "\n",
            "Missing Values:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               Count  Percentage\n",
              "other_income    4238   13.984491\n",
              "housing_spend   7356   24.273222"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72625c38-6689-4611-8a29-8e3ebef52670\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "      <th>Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>other_income</th>\n",
              "      <td>4238</td>\n",
              "      <td>13.984491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>housing_spend</th>\n",
              "      <td>7356</td>\n",
              "      <td>24.273222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72625c38-6689-4611-8a29-8e3ebef52670')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72625c38-6689-4611-8a29-8e3ebef52670 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72625c38-6689-4611-8a29-8e3ebef52670');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-52666795-a4fd-4f72-9e0d-a0bf53641cb7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52666795-a4fd-4f72-9e0d-a0bf53641cb7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-52666795-a4fd-4f72-9e0d-a0bf53641cb7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data_processor\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2204,\n        \"min\": 4238,\n        \"max\": 7356,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          7356,\n          4238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Percentage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.275231624285285,\n        \"min\": 13.984491008084474,\n        \"max\": 24.273222240554365,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          24.273222240554365,\n          13.984491008084474\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TARGET VARIABLE ANALYSIS: annual_net_savings_lbg\n",
            "==================================================\n",
            "Count: 30305\n",
            "Mean: $21,299.62\n",
            "Median: $22,870.70\n",
            "Std: $10,018.42\n",
            "Min: $0.00\n",
            "Max: $42,951.37\n",
            "\n",
            "Zero values: 3675 (12.13%)\n",
            "Positive values: 26630 (87.87%)\n",
            "\n",
            "Among Savers Only (n=26630):\n",
            "  Mean: $24,239.01\n",
            "  Median: $24,269.04\n",
            "  Std Dev: $6,555.13\n",
            "  Min: $6,302.42\n",
            "  Max: $42,951.37\n",
            "\n",
            "==================================================\n",
            "DATA TYPES\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "cust_unique_id              int64\n",
              "age                         int64\n",
              "gender                     object\n",
              "ethnicity_group            object\n",
              "geo_region                 object\n",
              "net_salary                float64\n",
              "other_income              float64\n",
              "observed_income           float64\n",
              "housing_spend             float64\n",
              "childcare_spend           float64\n",
              "gambling_spend            float64\n",
              "observed_surplus          float64\n",
              "credit_score               object\n",
              "savings_bal_lbg           float64\n",
              "annual_net_savings_lbg    float64\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cust_unique_id</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ethnicity_group</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>geo_region</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>net_salary</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other_income</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>observed_income</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>housing_spend</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>childcare_spend</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gambling_spend</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>observed_surplus</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>credit_score</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>savings_bal_lbg</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annual_net_savings_lbg</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic Statistics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       cust_unique_id           age     net_salary  other_income  \\\n",
              "count     30305.00000  30305.000000   30305.000000  26067.000000   \n",
              "mean      15153.00000     41.654182   56585.867472  13580.886176   \n",
              "std        8748.44429     15.444812   11794.832398   5862.046413   \n",
              "min           1.00000      4.000000   -1000.000000      0.000000   \n",
              "25%        7577.00000     29.000000   50407.530000   9962.795000   \n",
              "50%       15153.00000     42.000000   56643.520000  12716.980000   \n",
              "75%       22729.00000     55.000000   64805.760000  18749.150000   \n",
              "max       30305.00000     68.000000  104374.080000  29752.890000   \n",
              "\n",
              "       observed_income  housing_spend  childcare_spend  gambling_spend  \\\n",
              "count     30305.000000   22949.000000     30305.000000    30305.000000   \n",
              "mean      69246.199349   24719.267675       593.375517       98.408160   \n",
              "std       14708.576183    4325.260951      2528.447257     2881.362877   \n",
              "min       32518.450000   10573.570000         0.000000        0.000000   \n",
              "25%       59196.380000   21605.140000         0.000000        0.000000   \n",
              "50%       67338.610000   24603.170000         0.000000        0.000000   \n",
              "75%       82136.110000   28155.700000         0.000000        0.000000   \n",
              "max      131050.070000   42189.020000     17778.930000   448664.260000   \n",
              "\n",
              "       observed_surplus  savings_bal_lbg  annual_net_savings_lbg  \n",
              "count      30305.000000     3.030500e+04            30305.000000  \n",
              "mean       51270.967701     8.405509e+05            21299.618218  \n",
              "std        16050.305701     7.323375e+05            10018.424977  \n",
              "min      -408635.510000     0.000000e+00                0.000000  \n",
              "25%        40242.040000     1.514217e+05            16891.130000  \n",
              "50%        50093.010000     7.113819e+05            22870.700000  \n",
              "75%        60232.450000     1.397923e+06            28352.660000  \n",
              "max       128700.410000     3.615059e+06            42951.370000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a90f68df-0fbb-475a-97ee-20d160a1bf9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cust_unique_id</th>\n",
              "      <th>age</th>\n",
              "      <th>net_salary</th>\n",
              "      <th>other_income</th>\n",
              "      <th>observed_income</th>\n",
              "      <th>housing_spend</th>\n",
              "      <th>childcare_spend</th>\n",
              "      <th>gambling_spend</th>\n",
              "      <th>observed_surplus</th>\n",
              "      <th>savings_bal_lbg</th>\n",
              "      <th>annual_net_savings_lbg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>30305.00000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>26067.000000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>22949.000000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>30305.000000</td>\n",
              "      <td>3.030500e+04</td>\n",
              "      <td>30305.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15153.00000</td>\n",
              "      <td>41.654182</td>\n",
              "      <td>56585.867472</td>\n",
              "      <td>13580.886176</td>\n",
              "      <td>69246.199349</td>\n",
              "      <td>24719.267675</td>\n",
              "      <td>593.375517</td>\n",
              "      <td>98.408160</td>\n",
              "      <td>51270.967701</td>\n",
              "      <td>8.405509e+05</td>\n",
              "      <td>21299.618218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8748.44429</td>\n",
              "      <td>15.444812</td>\n",
              "      <td>11794.832398</td>\n",
              "      <td>5862.046413</td>\n",
              "      <td>14708.576183</td>\n",
              "      <td>4325.260951</td>\n",
              "      <td>2528.447257</td>\n",
              "      <td>2881.362877</td>\n",
              "      <td>16050.305701</td>\n",
              "      <td>7.323375e+05</td>\n",
              "      <td>10018.424977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>-1000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>32518.450000</td>\n",
              "      <td>10573.570000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-408635.510000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7577.00000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>50407.530000</td>\n",
              "      <td>9962.795000</td>\n",
              "      <td>59196.380000</td>\n",
              "      <td>21605.140000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40242.040000</td>\n",
              "      <td>1.514217e+05</td>\n",
              "      <td>16891.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15153.00000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>56643.520000</td>\n",
              "      <td>12716.980000</td>\n",
              "      <td>67338.610000</td>\n",
              "      <td>24603.170000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>50093.010000</td>\n",
              "      <td>7.113819e+05</td>\n",
              "      <td>22870.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22729.00000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>64805.760000</td>\n",
              "      <td>18749.150000</td>\n",
              "      <td>82136.110000</td>\n",
              "      <td>28155.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60232.450000</td>\n",
              "      <td>1.397923e+06</td>\n",
              "      <td>28352.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>30305.00000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>104374.080000</td>\n",
              "      <td>29752.890000</td>\n",
              "      <td>131050.070000</td>\n",
              "      <td>42189.020000</td>\n",
              "      <td>17778.930000</td>\n",
              "      <td>448664.260000</td>\n",
              "      <td>128700.410000</td>\n",
              "      <td>3.615059e+06</td>\n",
              "      <td>42951.370000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a90f68df-0fbb-475a-97ee-20d160a1bf9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a90f68df-0fbb-475a-97ee-20d160a1bf9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a90f68df-0fbb-475a-97ee-20d160a1bf9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-61e15fe4-bc39-41f7-989f-ae7904d7375d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61e15fe4-bc39-41f7-989f-ae7904d7375d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-61e15fe4-bc39-41f7-989f-ae7904d7375d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data_processor\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"cust_unique_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10921.763823652927,\n        \"min\": 1.0,\n        \"max\": 30305.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          30305.0,\n          15153.0,\n          22729.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10701.570727770386,\n        \"min\": 4.0,\n        \"max\": 30305.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          41.65418247813892,\n          42.0,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_salary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33008.46647904265,\n        \"min\": -1000.0,\n        \"max\": 104374.08,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          56585.8674723643,\n          56643.52,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"other_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9956.448058929423,\n        \"min\": 0.0,\n        \"max\": 29752.89,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          13580.886175624353,\n          12716.98,\n          26067.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"observed_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36569.25804191213,\n        \"min\": 14708.57618305789,\n        \"max\": 131050.07,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          69246.19934862234,\n          67338.61,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11354.416383541924,\n        \"min\": 4325.260950927606,\n        \"max\": 42189.02,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24719.267675279967,\n          24603.17,\n          22949.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"childcare_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11423.50257358956,\n        \"min\": 0.0,\n        \"max\": 30305.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          593.3755165814223,\n          17778.93,\n          2528.4472569632367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gambling_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 157294.9752161985,\n        \"min\": 0.0,\n        \"max\": 448664.26,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          98.40815970961889,\n          448664.26,\n          2881.3628773314244\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"observed_surplus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 166896.9885565149,\n        \"min\": -408635.51,\n        \"max\": 128700.41,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          51270.96770070946,\n          50093.01,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"savings_bal_lbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1183439.534562273,\n        \"min\": 0.0,\n        \"max\": 3615058.91,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          840550.892168289,\n          711381.9,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annual_net_savings_lbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13103.032062101998,\n        \"min\": 0.0,\n        \"max\": 42951.37,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          21299.618218115822,\n          22870.7,\n          30305.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking for zero-variance columns...\n",
            "  ‚úì No zero-variance columns found\n",
            "\n",
            "Duplicates: 0\n",
            "================================================== DATA CLEANING ==================================================\n",
            "\n",
            "‚ö†Ô∏è  Found 1166 records with age < 18\n",
            "   Age range: 4 to 17\n",
            "   Removing these records (likely data errors)...\n",
            "\n",
            "Handling missing values strategically:\n",
            "  ‚úì other_income: Imputing 4080 missing ‚Üí 0 (no other income)\n",
            "  ‚úì housing_spend: 7046 missing values detected\n",
            "     ‚Üí Creating 'has_housing_spend' indicator\n",
            "     ‚Üí Imputing with region-specific median\n",
            "\n",
            "Creating missingness indicators:\n",
            "  ‚úì Created 'has_housing_spend' indicator\n",
            "     ‚Üí 22093 with housing spend\n",
            "     ‚Üí 7046 without\n",
            "\n",
            "Engineering features:\n",
            "  ‚úì Added 4 features: ['savings_rate', 'age_group', 'discretionary_income', 'is_high_saver']\n",
            "\n",
            "Data Quality Validation:\n",
            "  ‚úì No missing values\n",
            "  ‚úì No infinite values\n",
            "  ‚úì Target variable is non-negative\n",
            "  ‚úì No duplicate rows\n",
            "\n",
            "==================================================\n",
            "PREPROCESSING SUMMARY\n",
            "==================================================\n",
            "Initial shape: (30305, 15)\n",
            "Final shape: (29139, 20)\n",
            "Rows removed: 1166\n",
            "Duplicates removed: 0\n",
            "Missing values remaining: 0\n",
            "================================================== DESCRIPTIVE ANALYSIS ==================================================\n",
            "\n",
            "Basic Statistics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       cust_unique_id           age     net_salary  other_income  \\\n",
              "count    29139.000000  29139.000000   29139.000000  29139.000000   \n",
              "mean     15161.979340     42.776348   56712.928649  11717.540273   \n",
              "std       8753.577151     14.634443   11786.091693   7183.197528   \n",
              "min          1.000000     18.000000   -1000.000000      0.000000   \n",
              "25%       7587.500000     30.000000   50540.965000   8505.270000   \n",
              "50%      15162.000000     43.000000   56750.780000  11585.310000   \n",
              "75%      22735.500000     55.000000   64954.390000  18342.105000   \n",
              "max      30305.000000     68.000000  104374.080000  29752.890000   \n",
              "\n",
              "       observed_income  housing_spend  childcare_spend  gambling_spend  \\\n",
              "count     29139.000000   29139.000000     29139.000000    29139.000000   \n",
              "mean      69407.806177   24726.135978       614.148615       99.461783   \n",
              "std       14681.319833    4081.574889      2569.888120     2936.907452   \n",
              "min       32518.450000   10573.570000         0.000000        0.000000   \n",
              "25%       59367.085000   21882.550000         0.000000        0.000000   \n",
              "50%       67475.800000   24348.000000         0.000000        0.000000   \n",
              "75%       82313.670000   28424.390000         0.000000        0.000000   \n",
              "max      131050.070000   42189.020000     17778.930000   448664.260000   \n",
              "\n",
              "       observed_surplus  savings_bal_lbg  annual_net_savings_lbg  \\\n",
              "count      29139.000000     2.913900e+04            29139.000000   \n",
              "mean       51419.007175     8.657010e+05            21499.901514   \n",
              "std        16029.755555     7.287584e+05            10034.326969   \n",
              "min      -408635.510000     0.000000e+00                0.000000   \n",
              "25%        40373.350000     1.953940e+05            17149.645000   \n",
              "50%        50299.970000     7.513822e+05            23207.030000   \n",
              "75%        60341.580000     1.419857e+06            28529.285000   \n",
              "max       128700.410000     3.615059e+06            42951.370000   \n",
              "\n",
              "       has_housing_spend  savings_rate  discretionary_income  is_high_saver  \n",
              "count       29139.000000  29139.000000          29139.000000   29139.000000  \n",
              "mean            0.758193      0.313791          44067.521584       0.439583  \n",
              "std             0.428185      0.141485          12042.267493       0.496345  \n",
              "min             0.000000      0.000000            182.590000       0.000000  \n",
              "25%             1.000000      0.267808          36160.375000       0.000000  \n",
              "50%             1.000000      0.328657          43308.480000       0.000000  \n",
              "75%             1.000000      0.404764          53611.600000       1.000000  \n",
              "max             1.000000      0.721682         100276.020000       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5169324f-cdfb-43d7-ad7c-fbe3cce21e13\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cust_unique_id</th>\n",
              "      <th>age</th>\n",
              "      <th>net_salary</th>\n",
              "      <th>other_income</th>\n",
              "      <th>observed_income</th>\n",
              "      <th>housing_spend</th>\n",
              "      <th>childcare_spend</th>\n",
              "      <th>gambling_spend</th>\n",
              "      <th>observed_surplus</th>\n",
              "      <th>savings_bal_lbg</th>\n",
              "      <th>annual_net_savings_lbg</th>\n",
              "      <th>has_housing_spend</th>\n",
              "      <th>savings_rate</th>\n",
              "      <th>discretionary_income</th>\n",
              "      <th>is_high_saver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>2.913900e+04</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "      <td>29139.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15161.979340</td>\n",
              "      <td>42.776348</td>\n",
              "      <td>56712.928649</td>\n",
              "      <td>11717.540273</td>\n",
              "      <td>69407.806177</td>\n",
              "      <td>24726.135978</td>\n",
              "      <td>614.148615</td>\n",
              "      <td>99.461783</td>\n",
              "      <td>51419.007175</td>\n",
              "      <td>8.657010e+05</td>\n",
              "      <td>21499.901514</td>\n",
              "      <td>0.758193</td>\n",
              "      <td>0.313791</td>\n",
              "      <td>44067.521584</td>\n",
              "      <td>0.439583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8753.577151</td>\n",
              "      <td>14.634443</td>\n",
              "      <td>11786.091693</td>\n",
              "      <td>7183.197528</td>\n",
              "      <td>14681.319833</td>\n",
              "      <td>4081.574889</td>\n",
              "      <td>2569.888120</td>\n",
              "      <td>2936.907452</td>\n",
              "      <td>16029.755555</td>\n",
              "      <td>7.287584e+05</td>\n",
              "      <td>10034.326969</td>\n",
              "      <td>0.428185</td>\n",
              "      <td>0.141485</td>\n",
              "      <td>12042.267493</td>\n",
              "      <td>0.496345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>-1000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>32518.450000</td>\n",
              "      <td>10573.570000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-408635.510000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>182.590000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7587.500000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>50540.965000</td>\n",
              "      <td>8505.270000</td>\n",
              "      <td>59367.085000</td>\n",
              "      <td>21882.550000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40373.350000</td>\n",
              "      <td>1.953940e+05</td>\n",
              "      <td>17149.645000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.267808</td>\n",
              "      <td>36160.375000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15162.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>56750.780000</td>\n",
              "      <td>11585.310000</td>\n",
              "      <td>67475.800000</td>\n",
              "      <td>24348.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>50299.970000</td>\n",
              "      <td>7.513822e+05</td>\n",
              "      <td>23207.030000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.328657</td>\n",
              "      <td>43308.480000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>22735.500000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>64954.390000</td>\n",
              "      <td>18342.105000</td>\n",
              "      <td>82313.670000</td>\n",
              "      <td>28424.390000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60341.580000</td>\n",
              "      <td>1.419857e+06</td>\n",
              "      <td>28529.285000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.404764</td>\n",
              "      <td>53611.600000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>30305.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>104374.080000</td>\n",
              "      <td>29752.890000</td>\n",
              "      <td>131050.070000</td>\n",
              "      <td>42189.020000</td>\n",
              "      <td>17778.930000</td>\n",
              "      <td>448664.260000</td>\n",
              "      <td>128700.410000</td>\n",
              "      <td>3.615059e+06</td>\n",
              "      <td>42951.370000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721682</td>\n",
              "      <td>100276.020000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5169324f-cdfb-43d7-ad7c-fbe3cce21e13')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5169324f-cdfb-43d7-ad7c-fbe3cce21e13 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5169324f-cdfb-43d7-ad7c-fbe3cce21e13');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cf784a54-bab9-4482-91a5-5a8973f48114\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf784a54-bab9-4482-91a5-5a8973f48114')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cf784a54-bab9-4482-91a5-5a8973f48114 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data_processor\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"cust_unique_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10711.79096344713,\n        \"min\": 1.0,\n        \"max\": 30305.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          15161.979340402897,\n          15162.0,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10288.499502736873,\n        \"min\": 14.634442634192327,\n        \"max\": 29139.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          42.77634784996054,\n          43.0,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_salary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33119.259185259485,\n        \"min\": -1000.0,\n        \"max\": 104374.08,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          56712.9286488898,\n          56750.78,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"other_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10540.170001140083,\n        \"min\": 0.0,\n        \"max\": 29752.89,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          11717.540272830227,\n          11585.31,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"observed_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36738.062994551954,\n        \"min\": 14681.319833331656,\n        \"max\": 131050.07,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          69407.80617694499,\n          67475.8,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11670.876374466512,\n        \"min\": 4081.574888887833,\n        \"max\": 42189.02,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24726.135978070626,\n          24348.0,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"childcare_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11073.632083074675,\n        \"min\": 0.0,\n        \"max\": 29139.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          614.1486145715364,\n          17778.93,\n          2569.8881202542275\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gambling_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 157324.27440070195,\n        \"min\": 0.0,\n        \"max\": 448664.26,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          99.46178317718523,\n          448664.26,\n          2936.9074524039975\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"observed_surplus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 166890.57646374864,\n        \"min\": -408635.51,\n        \"max\": 128700.41,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          51419.007175263396,\n          50299.97,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"savings_bal_lbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1179510.7541327733,\n        \"min\": 0.0,\n        \"max\": 3615058.91,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          865701.0455856412,\n          751382.15,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annual_net_savings_lbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13002.445142817287,\n        \"min\": 0.0,\n        \"max\": 42951.37,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          21499.901514121968,\n          23207.03,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_housing_spend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10301.930303172898,\n        \"min\": 0.0,\n        \"max\": 29139.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7581934863928069,\n          1.0,\n          0.4281850249347904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"savings_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10302.08223551706,\n        \"min\": 0.0,\n        \"max\": 29139.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3137905756137414,\n          0.32865729036120217,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"discretionary_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30108.248367484928,\n        \"min\": 182.59000000000378,\n        \"max\": 100276.02,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          44067.52158430282,\n          43308.48,\n          29139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_high_saver\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10302.04397014105,\n        \"min\": 0.0,\n        \"max\": 29139.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.43958268986581556,\n          1.0,\n          0.49634484305045246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TARGET VARIABLE ANALYSIS: annual_net_savings_lbg\n",
            "==================================================\n",
            "Count: 29139\n",
            "Mean: $21,499.90\n",
            "Median: $23,207.03\n",
            "Std: $10,034.33\n",
            "Min: $0.00\n",
            "Max: $42,951.37\n",
            "\n",
            "Zero values: 3521 (12.08%)\n",
            "Positive values: 25618 (87.92%)\n",
            "\n",
            "Among Savers Only (n=25618):\n",
            "  Mean: $24,454.90\n",
            "  Median: $24,540.14\n",
            "  Std Dev: $6,500.77\n",
            "  Min: $6,302.42\n",
            "  Max: $42,951.37\n",
            "\n",
            "‚úì Analysis outputs saved to 'analysis_outputs/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7a8d3cf"
      },
      "source": [
        "#### **Comment**\n",
        "\n",
        "Task 1 focused on data cleaning and descriptive analysis. Upon loading, the dataset contained 30305 records and 15 columns. Initial exploration revealed missing values in `other_income` (13.98%) and `housing_spend` (24.27%), which were imputed for columns with less than 5% missingness. No duplicate rows were found. Outliers in numeric columns were capped at the 1.5*IQR bounds, affecting 5624 values. An `age_group` feature was engineered. The final cleaned data has a shape of (30305, 16), with some remaining missing values.\n",
        "\n",
        "Descriptive analysis provided summary statistics (as displayed). Visualizations, including the target variable distribution, correlation matrix, and categorical vs. target variable box plots (as shown in the generated figures), offered insights into data characteristics and relationships.\n",
        "\n",
        ">   - OLU To fix: Make sure the summary above makes sense.\n",
        "    - Add rationale for dealing with missing data\n",
        "    - Add rationale for outliers\n",
        "    - Add rationale for feature engineering\n",
        "    - Add observation of negative values\n",
        "    - Add observation of break in constraint of minimum age required for opening account and with above point raising suspicion of validity of data source during collection.\n",
        "\n",
        "\n"
      ],
      "id": "a7a8d3cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2: Create Statistical Models**\n",
        "Sub-tasks\n",
        "- Train the statistical model(s)\n",
        "- Evaluate model performance using relevant metrics (e.g., R-squared, p-values)\n",
        "- Interpret model coefficients and assess statistical significance\n"
      ],
      "metadata": {
        "id": "RRoSiSFQ0jyZ"
      },
      "id": "RRoSiSFQ0jyZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "_AI6-8_jGMVl"
      },
      "id": "_AI6-8_jGMVl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba3b0958"
      },
      "source": [
        "class LBGModelBuilder:\n",
        "    \"\"\"Build and evaluate two-part statistical models with automatic feature selection.\"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, target_variable: str):\n",
        "        self.data = data\n",
        "        self.target_variable = target_variable\n",
        "        self.results = {}\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.X_clean = None\n",
        "        self.y_clean = None\n",
        "        self.is_saver = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_names = None\n",
        "\n",
        "    def build_statistical_models(self,\n",
        "                                 feature_selection_method: str = 'mutual_info',\n",
        "                                 max_features_logit: int = 30,\n",
        "                                 max_features_ols: int = 30,\n",
        "                                 use_robust_se: bool = True,\n",
        "                                 standardize: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Build a two-part model with automatic feature selection.\n",
        "        \"\"\"\n",
        "        print(\"=\"*70)\n",
        "        print(\"TWO-PART MODEL WITH FEATURE SELECTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self._prepare_model_data()\n",
        "        if self.X_clean is None or len(self.X_clean) == 0:\n",
        "            print(\"‚ùå No valid data after cleaning\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìä Dataset: {len(self.y_clean):,} obs, {self.X_clean.shape[1]} features\")\n",
        "\n",
        "        if standardize:\n",
        "            self.feature_names = self.X_clean.columns.tolist()\n",
        "            X_scaled = self.scaler.fit_transform(self.X_clean)\n",
        "            self.X_clean = pd.DataFrame(X_scaled, columns=self.feature_names, index=self.X_clean.index)\n",
        "\n",
        "        self._analyze_zero_inflation()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PART 1: LOGISTIC REGRESSION\")\n",
        "        print(f\"{'='*70}\")\n",
        "        X_logit = self._select_features_logit(method=feature_selection_method,\n",
        "                                               max_features=max_features_logit)\n",
        "        self._build_logistic_model(X_logit)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PART 2: OLS REGRESSION\")\n",
        "        print(f\"{'='*70}\")\n",
        "        X_ols = self._select_features_ols(method=feature_selection_method,\n",
        "                                           max_features=max_features_ols)\n",
        "        self._build_ols_model(X_ols, use_robust_se=use_robust_se)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"OLS-ONLY MODEL\")\n",
        "        print(f\"{'='*70}\")\n",
        "        self._build_ols_only_model(method=feature_selection_method,\n",
        "                                    max_features=max_features_ols,\n",
        "                                    use_robust_se=use_robust_se)\n",
        "\n",
        "        self._evaluate_two_part_model()\n",
        "        self._evaluate_ols_only_model()\n",
        "        self._generate_comparison_table()\n",
        "        self._plot_diagnostics()\n",
        "        self._generate_summary_report()\n",
        "\n",
        "    def _prepare_model_data(self, max_categories: int = 50) -> None:\n",
        "        \"\"\"Prepare data for modeling.\"\"\"\n",
        "        model_data = self.data.copy()\n",
        "\n",
        "        id_cols = [col for col in model_data.columns\n",
        "                   if 'id' in col.lower() or 'unique' in col.lower()]\n",
        "        if id_cols:\n",
        "            model_data = model_data.drop(columns=id_cols, errors='ignore')\n",
        "\n",
        "        for col in model_data.columns:\n",
        "            if col != self.target_variable and model_data[col].dtype == 'object':\n",
        "                try:\n",
        "                    converted = pd.to_numeric(model_data[col], errors='coerce')\n",
        "                    if converted.notna().sum() / len(converted) > 0.8:\n",
        "                        model_data[col] = converted\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        categorical_cols = model_data.select_dtypes(include=['object', 'category']).columns\n",
        "        categorical_cols = [col for col in categorical_cols if col != self.target_variable]\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            n_unique = model_data[col].nunique()\n",
        "            if n_unique > max_categories:\n",
        "                top_cats = model_data[col].value_counts().nlargest(max_categories).index\n",
        "                model_data[col] = model_data[col].where(\n",
        "                    model_data[col].isin(top_cats), other='_other_'\n",
        "                )\n",
        "            dummies = pd.get_dummies(model_data[col], prefix=col, drop_first=True, dtype=int)\n",
        "            model_data = pd.concat([model_data, dummies], axis=1)\n",
        "            model_data.drop(col, axis=1, inplace=True)\n",
        "\n",
        "        self.y = model_data[self.target_variable]\n",
        "        self.X = model_data.drop(self.target_variable, axis=1).select_dtypes(include=[np.number])\n",
        "\n",
        "        combined = pd.concat([self.X, self.y], axis=1)\n",
        "        combined = combined.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(combined) == 0:\n",
        "            return\n",
        "\n",
        "        self.X_clean = combined.iloc[:, :-1]\n",
        "        self.y_clean = combined.iloc[:, -1]\n",
        "        self.is_saver = (self.y_clean > 0).astype(int)\n",
        "\n",
        "    def _analyze_zero_inflation(self) -> None:\n",
        "        \"\"\"Analyze zero-inflation.\"\"\"\n",
        "        zero_count = (self.y_clean == 0).sum()\n",
        "        zero_pct = (zero_count / len(self.y_clean)) * 100\n",
        "\n",
        "        print(f\"\\nüìà Zero-inflation: {zero_count:,} non-savers ({zero_pct:.1f}%), \"\n",
        "              f\"{len(self.y_clean) - zero_count:,} savers ({100-zero_pct:.1f}%)\")\n",
        "\n",
        "        self.results['zero_inflation'] = {\n",
        "            'zero_count': zero_count,\n",
        "            'zero_percentage': zero_pct\n",
        "        }\n",
        "\n",
        "    def _select_features_logit(self, method: str = 'mutual_info', max_features: int = 30):\n",
        "        \"\"\"Select features for logistic regression.\"\"\"\n",
        "        correlations = self.X_clean.corrwith(self.is_saver).abs()\n",
        "        perfect_predictors = correlations[correlations > 0.70].index.tolist()\n",
        "\n",
        "        if perfect_predictors:\n",
        "            X_filtered = self.X_clean.drop(columns=perfect_predictors)\n",
        "        else:\n",
        "            X_filtered = self.X_clean.copy()\n",
        "\n",
        "        if X_filtered.shape[1] == 0:\n",
        "            return sm.add_constant(pd.DataFrame())\n",
        "\n",
        "        if method == 'mutual_info':\n",
        "            mi_scores = mutual_info_classif(X_filtered, self.is_saver, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "        elif method == 'f_test':\n",
        "            selector = SelectKBest(score_func=f_classif, k='all')\n",
        "            selector.fit(X_filtered, self.is_saver)\n",
        "            feature_scores = pd.Series(selector.scores_, index=X_filtered.columns)\n",
        "        elif method == 'correlation':\n",
        "            feature_scores = X_filtered.corrwith(self.is_saver).abs()\n",
        "        else:\n",
        "            mi_scores = mutual_info_classif(X_filtered, self.is_saver, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "\n",
        "        top_features = feature_scores.nlargest(max_features).index.tolist()\n",
        "        X_selected = X_filtered[top_features]\n",
        "\n",
        "        print(f\"\\n‚úì Selected {len(top_features)} features\")\n",
        "        print(f\"  Top 5: {', '.join(top_features[:5])}\")\n",
        "\n",
        "        self.results['logit_features'] = top_features\n",
        "        return sm.add_constant(X_selected)\n",
        "\n",
        "    def _select_features_ols(self, method: str = 'mutual_info', max_features: int = 30):\n",
        "        \"\"\"Select features for OLS regression on savers only.\"\"\"\n",
        "        savers_mask = self.y_clean > 0\n",
        "        X_savers = self.X_clean[savers_mask]\n",
        "        y_savers = self.y_clean[savers_mask]\n",
        "\n",
        "        correlations = X_savers.corrwith(y_savers).abs()\n",
        "        high_corr_features = correlations[correlations > 0.70].index.tolist()\n",
        "\n",
        "        if high_corr_features:\n",
        "            X_filtered = X_savers.drop(columns=high_corr_features)\n",
        "        else:\n",
        "            X_filtered = X_savers.copy()\n",
        "\n",
        "        if X_filtered.shape[1] == 0:\n",
        "            self.results['X_savers'] = pd.DataFrame()\n",
        "            self.results['y_savers'] = y_savers\n",
        "            return sm.add_constant(pd.DataFrame())\n",
        "\n",
        "        if method == 'mutual_info':\n",
        "            mi_scores = mutual_info_regression(X_filtered, y_savers, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "        elif method == 'f_test':\n",
        "            selector = SelectKBest(score_func=f_regression, k='all')\n",
        "            selector.fit(X_filtered, y_savers)\n",
        "            feature_scores = pd.Series(selector.scores_, index=X_filtered.columns)\n",
        "        elif method == 'lasso':\n",
        "            lasso = LassoCV(cv=5, random_state=42, max_iter=1000)\n",
        "            lasso.fit(X_filtered, y_savers)\n",
        "            feature_scores = pd.Series(np.abs(lasso.coef_), index=X_filtered.columns)\n",
        "        elif method == 'correlation':\n",
        "            feature_scores = X_filtered.corrwith(y_savers).abs()\n",
        "        else:\n",
        "            mi_scores = mutual_info_regression(X_filtered, y_savers, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "\n",
        "        top_features = feature_scores.nlargest(max_features).index.tolist()\n",
        "        X_selected = X_filtered[top_features]\n",
        "\n",
        "        print(f\"\\n‚úì Selected {len(top_features)} features\")\n",
        "        print(f\"  Top 5: {', '.join(top_features[:5])}\")\n",
        "\n",
        "        self.results['ols_features'] = top_features\n",
        "        self.results['X_savers'] = X_selected\n",
        "        self.results['y_savers'] = y_savers\n",
        "\n",
        "        return sm.add_constant(X_selected)\n",
        "\n",
        "    def _build_logistic_model(self, X_with_const) -> None:\n",
        "        \"\"\"Build logistic regression model.\"\"\"\n",
        "        if X_with_const.shape[1] <= 1:\n",
        "            self.results['logit_model'] = None\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            logit_model = Logit(self.is_saver, X_with_const)\n",
        "            logit_results = logit_model.fit(method='bfgs', maxiter=100, disp=False)\n",
        "\n",
        "            prob_save = logit_results.predict(X_with_const)\n",
        "            pred_class = (prob_save > 0.5).astype(int)\n",
        "            accuracy = (pred_class == self.is_saver).mean()\n",
        "\n",
        "            print(f\"\\n  Pseudo R¬≤: {logit_results.prsquared:.4f}\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            try:\n",
        "                auc = roc_auc_score(self.is_saver, prob_save)\n",
        "                print(f\"  ROC-AUC: {auc:.4f}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            sig_count = (logit_results.pvalues < 0.05).sum() - 1\n",
        "            print(f\"  Significant predictors: {sig_count}\")\n",
        "\n",
        "            self.results['logit_model'] = logit_results\n",
        "            self.results['logit_predictions'] = prob_save\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Logistic model failed: {e}\")\n",
        "            self.results['logit_model'] = None\n",
        "\n",
        "    def _build_ols_model(self, X_savers_const, use_robust_se: bool = True) -> None:\n",
        "        \"\"\"Build OLS regression model on savers only.\"\"\"\n",
        "        if X_savers_const.shape[1] <= 1:\n",
        "            self.results['ols_model'] = None\n",
        "            return\n",
        "\n",
        "        y_savers = self.results['y_savers']\n",
        "\n",
        "        print(f\"\\n  Sample: {len(y_savers):,} savers\")\n",
        "\n",
        "        try:\n",
        "            ols_model = OLS(y_savers, X_savers_const)\n",
        "\n",
        "            if use_robust_se:\n",
        "                ols_results = ols_model.fit(cov_type='HC3')\n",
        "            else:\n",
        "                ols_results = ols_model.fit()\n",
        "\n",
        "            print(f\"  R¬≤: {ols_results.rsquared:.4f}\")\n",
        "            print(f\"  Adjusted R¬≤: {ols_results.rsquared_adj:.4f}\")\n",
        "\n",
        "            sig_count = (ols_results.pvalues < 0.05).sum() - 1\n",
        "            print(f\"  Significant predictors: {sig_count}\")\n",
        "\n",
        "            self.results['ols_model'] = ols_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå OLS model failed: {e}\")\n",
        "            self.results['ols_model'] = None\n",
        "\n",
        "    def _build_ols_only_model(self, method: str = 'mutual_info', max_features: int = 30,\n",
        "                               use_robust_se: bool = True) -> None:\n",
        "        \"\"\"Build OLS regression on full dataset.\"\"\"\n",
        "        correlations = self.X_clean.corrwith(self.y_clean).abs()\n",
        "        high_corr_features = correlations[correlations > 0.70].index.tolist()\n",
        "\n",
        "        if high_corr_features:\n",
        "            X_filtered = self.X_clean.drop(columns=high_corr_features)\n",
        "        else:\n",
        "            X_filtered = self.X_clean.copy()\n",
        "\n",
        "        if X_filtered.shape[1] == 0:\n",
        "            self.results['ols_only_model'] = None\n",
        "            return\n",
        "\n",
        "        if method == 'mutual_info':\n",
        "            mi_scores = mutual_info_regression(X_filtered, self.y_clean, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "        elif method == 'f_test':\n",
        "            selector = SelectKBest(score_func=f_regression, k='all')\n",
        "            selector.fit(X_filtered, self.y_clean)\n",
        "            feature_scores = pd.Series(selector.scores_, index=X_filtered.columns)\n",
        "        elif method == 'lasso':\n",
        "            lasso = LassoCV(cv=5, random_state=42, max_iter=1000)\n",
        "            lasso.fit(X_filtered, self.y_clean)\n",
        "            feature_scores = pd.Series(np.abs(lasso.coef_), index=X_filtered.columns)\n",
        "        elif method == 'correlation':\n",
        "            feature_scores = X_filtered.corrwith(self.y_clean).abs()\n",
        "        else:\n",
        "            mi_scores = mutual_info_regression(X_filtered, self.y_clean, random_state=42)\n",
        "            feature_scores = pd.Series(mi_scores, index=X_filtered.columns)\n",
        "\n",
        "        top_features = feature_scores.nlargest(max_features).index.tolist()\n",
        "        X_selected = X_filtered[top_features]\n",
        "\n",
        "        print(f\"\\n‚úì Selected {len(top_features)} features\")\n",
        "        print(f\"  Top 5: {', '.join(top_features[:5])}\")\n",
        "\n",
        "        X_with_const = sm.add_constant(X_selected)\n",
        "\n",
        "        try:\n",
        "            ols_model = OLS(self.y_clean, X_with_const)\n",
        "\n",
        "            if use_robust_se:\n",
        "                ols_results = ols_model.fit(cov_type='HC3')\n",
        "            else:\n",
        "                ols_results = ols_model.fit()\n",
        "\n",
        "            print(f\"  R¬≤: {ols_results.rsquared:.4f}\")\n",
        "            print(f\"  Adjusted R¬≤: {ols_results.rsquared_adj:.4f}\")\n",
        "\n",
        "            sig_count = (ols_results.pvalues < 0.05).sum() - 1\n",
        "            print(f\"  Significant predictors: {sig_count}\")\n",
        "\n",
        "            self.results['ols_only_model'] = ols_results\n",
        "            self.results['ols_only_features'] = top_features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå OLS-only model failed: {e}\")\n",
        "            self.results['ols_only_model'] = None\n",
        "\n",
        "    def _evaluate_two_part_model(self) -> None:\n",
        "        \"\"\"Evaluate combined two-part model.\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"TWO-PART MODEL EVALUATION\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if self.results.get('logit_model') is None or self.results.get('ols_model') is None:\n",
        "            print(\"  ‚ùå Cannot evaluate\")\n",
        "            return\n",
        "\n",
        "        logit_features = self.results.get('logit_features', [])\n",
        "        X_logit_full = sm.add_constant(self.X_clean[logit_features])\n",
        "        prob_save = self.results['logit_model'].predict(X_logit_full)\n",
        "\n",
        "        ols_features = self.results.get('ols_features', [])\n",
        "        X_ols_full = sm.add_constant(self.X_clean[ols_features])\n",
        "        amount_pred = self.results['ols_model'].predict(X_ols_full)\n",
        "        amount_pred = np.maximum(amount_pred, 0)\n",
        "\n",
        "        final_prediction = prob_save * amount_pred\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(self.y_clean, final_prediction))\n",
        "        mae = mean_absolute_error(self.y_clean, final_prediction)\n",
        "        r2 = r2_score(self.y_clean, final_prediction)\n",
        "\n",
        "        baseline_rmse = np.sqrt(mean_squared_error(self.y_clean,\n",
        "                                                    np.full(len(self.y_clean), self.y_clean.mean())))\n",
        "        improvement = (1 - rmse / baseline_rmse) * 100\n",
        "\n",
        "        print(f\"\\n  RMSE: ¬£{rmse:,.2f}\")\n",
        "        print(f\"  MAE: ¬£{mae:,.2f}\")\n",
        "        print(f\"  R¬≤: {r2:.4f}\")\n",
        "        print(f\"  Improvement: {improvement:+.1f}%\")\n",
        "\n",
        "        self.results['final_predictions'] = final_prediction\n",
        "        self.results['metrics'] = {\n",
        "            'rmse': rmse, 'mae': mae, 'r2': r2,\n",
        "            'baseline_rmse': baseline_rmse, 'improvement_pct': improvement\n",
        "        }\n",
        "\n",
        "    def _evaluate_ols_only_model(self) -> None:\n",
        "        \"\"\"Evaluate OLS-only model.\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"OLS-ONLY MODEL EVALUATION\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if self.results.get('ols_only_model') is None:\n",
        "            print(\"  ‚ùå Cannot evaluate\")\n",
        "            return\n",
        "\n",
        "        ols_only_features = self.results.get('ols_only_features', [])\n",
        "        X_ols_only = sm.add_constant(self.X_clean[ols_only_features])\n",
        "        ols_only_pred = self.results['ols_only_model'].predict(X_ols_only)\n",
        "        ols_only_pred = np.maximum(ols_only_pred, 0)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(self.y_clean, ols_only_pred))\n",
        "        mae = mean_absolute_error(self.y_clean, ols_only_pred)\n",
        "        r2 = r2_score(self.y_clean, ols_only_pred)\n",
        "\n",
        "        baseline_rmse = np.sqrt(mean_squared_error(self.y_clean,\n",
        "                                                    np.full(len(self.y_clean), self.y_clean.mean())))\n",
        "        improvement = (1 - rmse / baseline_rmse) * 100\n",
        "\n",
        "        print(f\"\\n  RMSE: ¬£{rmse:,.2f}\")\n",
        "        print(f\"  MAE: ¬£{mae:,.2f}\")\n",
        "        print(f\"  R¬≤: {r2:.4f}\")\n",
        "        print(f\"  Improvement: {improvement:+.1f}%\")\n",
        "\n",
        "        self.results['ols_only_predictions'] = ols_only_pred\n",
        "        self.results['ols_only_metrics'] = {\n",
        "            'rmse': rmse, 'mae': mae, 'r2': r2,\n",
        "            'baseline_rmse': baseline_rmse, 'improvement_pct': improvement\n",
        "        }\n",
        "\n",
        "    def _generate_comparison_table(self) -> None:\n",
        "        \"\"\"Generate comparison table of all models.\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"MODEL COMPARISON\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        baseline_pred = np.full(len(self.y_clean), self.y_clean.mean())\n",
        "        baseline_rmse = np.sqrt(mean_squared_error(self.y_clean, baseline_pred))\n",
        "        baseline_mae = mean_absolute_error(self.y_clean, baseline_pred)\n",
        "        baseline_r2 = r2_score(self.y_clean, baseline_pred)\n",
        "\n",
        "        comparison_data = []\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Model': 'Baseline (Mean)',\n",
        "            'RMSE (¬£)': baseline_rmse,\n",
        "            'MAE (¬£)': baseline_mae,\n",
        "            'R¬≤': baseline_r2,\n",
        "            'RMSE Improvement (%)': 0.0,\n",
        "            'MAE Improvement (%)': 0.0\n",
        "        })\n",
        "\n",
        "        if self.results.get('ols_only_metrics'):\n",
        "            m = self.results['ols_only_metrics']\n",
        "            rmse_imp = ((baseline_rmse - m['rmse']) / baseline_rmse) * 100\n",
        "            mae_imp = ((baseline_mae - m['mae']) / baseline_mae) * 100\n",
        "            comparison_data.append({\n",
        "                'Model': 'OLS Only',\n",
        "                'RMSE (¬£)': m['rmse'],\n",
        "                'MAE (¬£)': m['mae'],\n",
        "                'R¬≤': m['r2'],\n",
        "                'RMSE Improvement (%)': rmse_imp,\n",
        "                'MAE Improvement (%)': mae_imp\n",
        "            })\n",
        "\n",
        "        if self.results.get('metrics'):\n",
        "            m = self.results['metrics']\n",
        "            rmse_imp = ((baseline_rmse - m['rmse']) / baseline_rmse) * 100\n",
        "            mae_imp = ((baseline_mae - m['mae']) / baseline_mae) * 100\n",
        "            comparison_data.append({\n",
        "                'Model': 'Two-Part (Logit+OLS)',\n",
        "                'RMSE (¬£)': m['rmse'],\n",
        "                'MAE (¬£)': m['mae'],\n",
        "                'R¬≤': m['r2'],\n",
        "                'RMSE Improvement (%)': rmse_imp,\n",
        "                'MAE Improvement (%)': mae_imp\n",
        "            })\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        print(comparison_df.to_string(index=False, float_format=lambda x: f'{x:,.2f}'))\n",
        "        print(f\"\\n{'='*70}\")\n",
        "\n",
        "        self.results['comparison_table'] = comparison_df\n",
        "\n",
        "        output_dir = Path(\"analysis_outputs\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        comparison_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
        "        print(f\"\\nüíæ Saved: {output_dir / 'model_comparison.csv'}\")\n",
        "\n",
        "        best_r2_idx = comparison_df['R¬≤'].idxmax()\n",
        "        best_rmse_idx = comparison_df['RMSE (¬£)'].idxmin()\n",
        "\n",
        "        print(f\"\\nüèÜ Best R¬≤: {comparison_df.loc[best_r2_idx, 'Model']} ({comparison_df.loc[best_r2_idx, 'R¬≤']:.4f})\")\n",
        "        print(f\"üèÜ Best RMSE: {comparison_df.loc[best_rmse_idx, 'Model']} (¬£{comparison_df.loc[best_rmse_idx, 'RMSE (¬£)']:,.2f})\")\n",
        "\n",
        "    def _plot_diagnostics(self) -> None:\n",
        "        \"\"\"Generate diagnostic plots.\"\"\"\n",
        "        output_dir = Path(\"analysis_outputs\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
        "            fig.suptitle('Model Diagnostics', fontsize=16, fontweight='bold')\n",
        "\n",
        "            if 'final_predictions' in self.results:\n",
        "                ax = axes[0, 0]\n",
        "                ax.scatter(self.y_clean, self.results['final_predictions'],\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                max_val = max(self.y_clean.max(), self.results['final_predictions'].max())\n",
        "                ax.plot([0, max_val], [0, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "                ax.set_xlabel('Actual Savings (¬£)')\n",
        "                ax.set_ylabel('Predicted Savings (¬£)')\n",
        "                ax.set_title('Two-Part Model: Actual vs Predicted')\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            if 'final_predictions' in self.results:\n",
        "                ax = axes[0, 1]\n",
        "                residuals = self.y_clean - self.results['final_predictions']\n",
        "                ax.scatter(self.results['final_predictions'], residuals,\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "                ax.set_xlabel('Predicted Savings (¬£)')\n",
        "                ax.set_ylabel('Residuals (¬£)')\n",
        "                ax.set_title('Two-Part Model: Residual Plot')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            ax = axes[1, 0]\n",
        "            ax.hist(self.y_clean, bins=50, alpha=0.6, label='Actual', edgecolor='black')\n",
        "            if 'final_predictions' in self.results:\n",
        "                ax.hist(self.results['final_predictions'], bins=50, alpha=0.6,\n",
        "                       label='Two-Part', edgecolor='black')\n",
        "            if 'ols_only_predictions' in self.results:\n",
        "                ax.hist(self.results['ols_only_predictions'], bins=50, alpha=0.6,\n",
        "                       label='OLS-Only', edgecolor='black')\n",
        "            ax.set_xlabel('Savings (¬£)')\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.set_title('Distribution Comparison')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "            if 'comparison_table' in self.results:\n",
        "                ax = axes[1, 1]\n",
        "                comp_df = self.results['comparison_table']\n",
        "                models = comp_df['Model'].tolist()\n",
        "                r2_values = comp_df['R¬≤'].tolist()\n",
        "\n",
        "                bars = ax.barh(models, r2_values, color=['lightgray', 'skyblue', 'lightgreen'])\n",
        "                ax.set_xlabel('R¬≤ Score')\n",
        "                ax.set_title('Model Performance (R¬≤)')\n",
        "                ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "                for i, (bar, val) in enumerate(zip(bars, r2_values)):\n",
        "                    ax.text(val + 0.01, i, f'{val:.4f}', va='center')\n",
        "\n",
        "            if 'ols_only_predictions' in self.results and 'ols_only_model' in self.results:\n",
        "                ols_only_pred = self.results['ols_only_predictions']\n",
        "                ols_only_residuals = self.y_clean - ols_only_pred\n",
        "\n",
        "                ax = axes[2, 0]\n",
        "                ax.scatter(ols_only_pred, ols_only_residuals,\n",
        "                          alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
        "                ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "                ax.set_xlabel('Fitted Values (¬£)')\n",
        "                ax.set_ylabel('Residuals (¬£)')\n",
        "                ax.set_title('OLS-Only: Residuals vs Fitted')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                ax = axes[2, 1]\n",
        "                standardized_residuals = ols_only_residuals / ols_only_residuals.std()\n",
        "                stats.probplot(standardized_residuals, dist=\"norm\", plot=ax)\n",
        "                ax.set_title('OLS-Only: Q-Q Plot')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plot_path = output_dir / 'model_diagnostics.png'\n",
        "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"\\nüíæ Saved: {plot_path}\")\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Could not generate plots: {e}\")\n",
        "\n",
        "    def _generate_summary_report(self) -> None:\n",
        "        \"\"\"Generate summary report.\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ANALYSIS COMPLETE\")\n",
        "        print(f\"{'='*70}\\n\")"
      ],
      "id": "ba3b0958",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "382e94b2",
        "outputId": "76408236-d809-47b6-fd76-324365ec429d"
      },
      "source": [
        "# Run two-part statistical models\n",
        "if data_processor.clean_data is not None:\n",
        "    model_builder = LBGModelBuilder(data_processor.clean_data, data_processor.target_variable)\n",
        "    model_builder.build_statistical_models(use_robust_se=True)\n",
        "\n",
        "    # Optional: Get detailed model summaries\n",
        "    # model_builder.get_model_summaries()\n",
        "else:\n",
        "    print(\"Data not loaded or cleaned. Please run previous steps.\")\n",
        "\n",
        "# Access results\n",
        "logit_results = model_builder.results['logit_model']\n",
        "ols_results = model_builder.results['ols_model']\n",
        "predictions = model_builder.results['final_predictions']"
      ],
      "id": "382e94b2",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TWO-PART MODEL WITH FEATURE SELECTION\n",
            "======================================================================\n",
            "\n",
            "üìä Dataset: 28,134 obs, 30 features\n",
            "\n",
            "üìà Zero-inflation: 3,386 non-savers (12.0%), 24,748 savers (88.0%)\n",
            "\n",
            "======================================================================\n",
            "PART 1: LOGISTIC REGRESSION\n",
            "======================================================================\n",
            "\n",
            "‚úì Selected 29 features\n",
            "  Top 5: is_high_saver, gender_M, geo_region_Southeast, gender_F, other_income\n",
            "\n",
            "  Pseudo R¬≤: 0.3384\n",
            "  Accuracy: 0.9064\n",
            "  ROC-AUC: 0.8635\n",
            "  Significant predictors: 6\n",
            "\n",
            "======================================================================\n",
            "PART 2: OLS REGRESSION\n",
            "======================================================================\n",
            "\n",
            "‚úì Selected 27 features\n",
            "  Top 5: savings_rate, credit_score, discretionary_income, observed_surplus, net_salary\n",
            "\n",
            "  Sample: 24,748 savers\n",
            "  R¬≤: 0.9729\n",
            "  Adjusted R¬≤: 0.9728\n",
            "  Significant predictors: 14\n",
            "\n",
            "======================================================================\n",
            "OLS-ONLY MODEL\n",
            "======================================================================\n",
            "\n",
            "‚úì Selected 28 features\n",
            "  Top 5: savings_bal_lbg, age, credit_score, discretionary_income, observed_surplus\n",
            "  R¬≤: 0.2717\n",
            "  Adjusted R¬≤: 0.2710\n",
            "  Significant predictors: 7\n",
            "\n",
            "======================================================================\n",
            "TWO-PART MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "  RMSE: ¬£3,128.99\n",
            "  MAE: ¬£2,024.26\n",
            "  R¬≤: 0.9026\n",
            "  Improvement: +68.8%\n",
            "\n",
            "======================================================================\n",
            "OLS-ONLY MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "  RMSE: ¬£8,555.31\n",
            "  MAE: ¬£5,505.69\n",
            "  R¬≤: 0.2717\n",
            "  Improvement: +14.7%\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON\n",
            "======================================================================\n",
            "\n",
            "               Model  RMSE (¬£)  MAE (¬£)   R¬≤  RMSE Improvement (%)  MAE Improvement (%)\n",
            "     Baseline (Mean) 10,025.17 7,739.42 0.00                  0.00                 0.00\n",
            "            OLS Only  8,555.31 5,505.69 0.27                 14.66                28.86\n",
            "Two-Part (Logit+OLS)  3,128.99 2,024.26 0.90                 68.79                73.84\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üíæ Saved: analysis_outputs/model_comparison.csv\n",
            "\n",
            "üèÜ Best R¬≤: Two-Part (Logit+OLS) (0.9026)\n",
            "üèÜ Best RMSE: Two-Part (Logit+OLS) (¬£3,128.99)\n",
            "\n",
            "üíæ Saved: analysis_outputs/model_diagnostics.png\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS COMPLETE\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f23e2e"
      },
      "source": [
        "#### **Comment**\n",
        "\n",
        "Based on the evaluation metrics, the two-part model (Logistic + OLS) significantly outperforms the OLS-only model in predicting annual net savings.\n",
        "\n",
        "| Model                | RMSE (¬£)   | MAE (¬£)    | R¬≤     | RMSE Improvement (%) | MAE Improvement (%) |\n",
        "|----------------------|------------|------------|--------|----------------------|---------------------|\n",
        "| Baseline (Mean)      | 10,025.17  | 7,739.42   | 0.00   | 0.00                 | 0.00                |\n",
        "| OLS Only             | 8,555.31   | 5,505.69   | 0.27   | 14.66                | 28.86               |\n",
        "| Two-Part (Logit+OLS) | 3,128.99   | 2,024.26   | 0.90   | 68.79                | 73.84               |\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **RMSE and MAE:** The Two-Part model has a much lower RMSE and MAE, indicating its predictions are closer to the actual savings values on average.\n",
        "*   **R¬≤:** The Two-Part model explains a significantly higher proportion of the variance in annual net savings (R¬≤ = 0.90) compared to the OLS-only model (R¬≤ = 0.27).\n",
        "*   **Improvement:** The Two-Part model provides a substantial improvement over the baseline (mean prediction) in both RMSE and MAE, demonstrating its superior predictive power.\n",
        "\n",
        "This suggests that explicitly modeling the decision to save (Part 1: Logistic) and then the amount saved (Part 2: OLS) provides a more accurate prediction of annual net savings than a single OLS model, particularly given the zero-inflation observed in the target variable."
      ],
      "id": "e4f23e2e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3: Create ML Models**\n",
        "Sub-tasks\n",
        "- Train the ML models and evaluate performance using predictive metrics (e.g., RMSE, MAE)\n",
        "- Identify and analyze ethical concerns related to predictor variables\n",
        "- Explore and potentially implement fairness mitigation techniques\n"
      ],
      "metadata": {
        "id": "UK_oq2wq0jvP"
      },
      "id": "UK_oq2wq0jvP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "Mq_-XS-w0jsz"
      },
      "id": "Mq_-XS-w0jsz"
    },
    {
      "cell_type": "code",
      "source": [
        "class LBGMLModelBuilder:\n",
        "    \"\"\"Build and evaluate machine learning models for savings prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, target_variable: str):\n",
        "        self.clean_data = data\n",
        "        self.target_variable = target_variable\n",
        "        self.results = {}\n",
        "        self.data_dictionary: Optional[pd.DataFrame] = None\n",
        "\n",
        "    def _prepare_model_data(self, max_categories: int = 50) -> None:\n",
        "        \"\"\"Prepare data for modeling using utility functions.\"\"\"\n",
        "        model_data = self.clean_data.copy()\n",
        "\n",
        "        # Use utility functions for data preparation\n",
        "        model_data = Utilities.remove_id_columns(model_data)\n",
        "        model_data = Utilities.convert_numeric_strings(model_data, self.target_variable)\n",
        "        model_data = Utilities.encode_categorical_features(model_data, self.target_variable, max_categories)\n",
        "\n",
        "        # Separate features and target\n",
        "        y = model_data[self.target_variable]\n",
        "        X = model_data.drop(self.target_variable, axis=1).select_dtypes(include=[np.number])\n",
        "\n",
        "        # Clean data\n",
        "        self.X_clean, self.y_clean = Utilities.clean_infinite_and_nan(X, y)\n",
        "\n",
        "    def build_ml_models(self) -> None:\n",
        "        print(\"=\"*50, \"MACHINE LEARNING\", \"=\"*50)\n",
        "\n",
        "        if self.target_variable not in self.clean_data.columns:\n",
        "            print(\"Target variable not found. Skipping.\")\n",
        "            return\n",
        "\n",
        "        self._prepare_model_data()\n",
        "\n",
        "        if self.X_clean.empty or self.y_clean.empty:\n",
        "            print(\"Data preparation failed or resulted in empty data. Skipping model building.\")\n",
        "            return\n",
        "\n",
        "        # Train/test split using utility\n",
        "        X_train, X_test, y_train, y_test = Utilities.stratified_train_test_split(\n",
        "            self.X_clean, self.y_clean, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = pd.DataFrame(\n",
        "            scaler.fit_transform(X_train),\n",
        "            columns=X_train.columns,\n",
        "            index=X_train.index\n",
        "        )\n",
        "        X_test_scaled = pd.DataFrame(\n",
        "            scaler.transform(X_test),\n",
        "            columns=X_test.columns,\n",
        "            index=X_test.index\n",
        "        )\n",
        "\n",
        "        print(f\"Training data shape: {X_train_scaled.shape}, Test data shape: {X_test_scaled.shape}\")\n",
        "\n",
        "        ml_results = {}\n",
        "\n",
        "        # Define all models\n",
        "        models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge': Ridge(),\n",
        "            'Lasso': Lasso(),\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate all models\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            y_pred = np.maximum(model.predict(X_test_scaled), 0)\n",
        "\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            ml_results[name] = {'rmse': rmse, 'mae': mae, 'r2': r2, 'model': model, 'predictions': y_pred}\n",
        "            print(f\"  RMSE: ¬£{rmse:,.2f}, MAE: ¬£{mae:,.2f}, R¬≤: {r2:.4f}\")\n",
        "\n",
        "        self.results['ml_models'] = ml_results\n",
        "        self.results['y_test'] = y_test\n",
        "\n",
        "        print(\"\\nML Model Training Complete.\")\n",
        "\n",
        "    def evaluate_ml_models(self) -> None:\n",
        "        \"\"\"Evaluate ML models and compare with statistical models.\"\"\"\n",
        "        print(\"=\"*50, \"ML MODEL EVALUATION\", \"=\"*50)\n",
        "\n",
        "        if 'ml_models' not in self.results or not self.results['ml_models']:\n",
        "            print(\"No ML models found. Please run build_ml_models first.\")\n",
        "            return\n",
        "\n",
        "        comparison_data = self._prepare_comparison_data()\n",
        "        comparison_df = pd.DataFrame(comparison_data).sort_values(by='RMSE (¬£)')\n",
        "\n",
        "        print(\"\\nModel Comparison Table:\")\n",
        "        display(comparison_df.round(2).style.format({\n",
        "            'RMSE (¬£)': '¬£{:,.2f}',\n",
        "            'MAE (¬£)': '¬£{:,.2f}',\n",
        "            'R¬≤': '{:.4f}'\n",
        "        }))\n",
        "\n",
        "        self.results['ml_comparison_table'] = comparison_df\n",
        "        Utilities.save_dataframe(comparison_df, 'ml_model_comparison.csv')\n",
        "\n",
        "    def _prepare_comparison_data(self) -> List[Dict]:\n",
        "        \"\"\"Prepare comparison data from ML and statistical models.\"\"\"\n",
        "        comparison_data = []\n",
        "\n",
        "        # Add ML model results\n",
        "        for name, metrics in self.results['ml_models'].items():\n",
        "            comparison_data.append({\n",
        "                'Model': name,\n",
        "                'RMSE (¬£)': metrics['rmse'],\n",
        "                'MAE (¬£)': metrics['mae'],\n",
        "                'R¬≤': metrics['r2'],\n",
        "                'Type': 'Machine Learning'\n",
        "            })\n",
        "\n",
        "        # Add statistical model results if available\n",
        "        try:\n",
        "            if 'model_builder' in globals() and model_builder.results.get('comparison_table') is not None:\n",
        "                stat_comp_df = model_builder.results['comparison_table']\n",
        "                for index, row in stat_comp_df.iterrows():\n",
        "                    if row['Model'] in ['OLS Only', 'Two-Part (Logit+OLS)', 'Baseline (Mean)']:\n",
        "                        comparison_data.append({\n",
        "                            'Model': row['Model'],\n",
        "                            'RMSE (¬£)': row['RMSE (¬£)'],\n",
        "                            'MAE (¬£)': row['MAE (¬£)'],\n",
        "                            'R¬≤': row['R¬≤'],\n",
        "                            'Type': row['Model'].split(' ')[0] if row['Model'] != 'Baseline (Mean)' else 'Baseline'\n",
        "                        })\n",
        "        except NameError:\n",
        "            print(\"Warning: Statistical model results not found in global scope.\")\n",
        "\n",
        "        return comparison_data\n",
        "\n",
        "    def get_feature_importance(self) -> pd.DataFrame:\n",
        "        \"\"\"Extract and return feature coefficients/importances from the best ML model.\"\"\"\n",
        "\n",
        "        if 'ml_models' not in self.results or not self.results['ml_models']:\n",
        "            print(\"No ML models found. Please run build_ml_models first.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Find best model\n",
        "        best_model_name = min(self.results['ml_models'],\n",
        "                              key=lambda x: self.results['ml_models'][x]['rmse'])\n",
        "        best_model = self.results['ml_models'][best_model_name]['model']\n",
        "\n",
        "        print(f\"\\n{'='*50} FEATURE IMPORTANCE {'='*50}\")\n",
        "        print(f\"Best Model: {best_model_name}\")\n",
        "        print(f\"RMSE: ¬£{self.results['ml_models'][best_model_name]['rmse']:,.2f}\")\n",
        "        print(f\"R¬≤: {self.results['ml_models'][best_model_name]['r2']:.4f}\\n\")\n",
        "\n",
        "        feature_importance_df = self._extract_feature_importance(best_model, best_model_name)\n",
        "\n",
        "        if feature_importance_df.empty:\n",
        "            return feature_importance_df\n",
        "\n",
        "        # Save results\n",
        "        self._save_feature_importance(feature_importance_df, best_model, best_model_name)\n",
        "\n",
        "        return feature_importance_df\n",
        "\n",
        "    def _extract_feature_importance(self, model, model_name: str) -> pd.DataFrame:\n",
        "        \"\"\"Extract feature importance from model.\"\"\"\n",
        "        feature_names = self.X_clean.columns.tolist()\n",
        "\n",
        "        if hasattr(model, 'coef_'):\n",
        "            importance_values = model.coef_\n",
        "            importance_type = 'Coefficient'\n",
        "            if hasattr(model, 'intercept_'):\n",
        "                print(f\"Intercept: ¬£{model.intercept_:,.2f}\\n\")\n",
        "        elif hasattr(model, 'feature_importances_'):\n",
        "            importance_values = model.feature_importances_\n",
        "            importance_type = 'Importance'\n",
        "        else:\n",
        "            print(\"Model doesn't have coefficients or feature importances.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            importance_type: importance_values\n",
        "        })\n",
        "\n",
        "        # Sort appropriately\n",
        "        if importance_type == 'Coefficient':\n",
        "            feature_importance_df['Abs_Value'] = feature_importance_df[importance_type].abs()\n",
        "            feature_importance_df = feature_importance_df.sort_values('Abs_Value', ascending=False)\n",
        "            feature_importance_df = feature_importance_df.drop('Abs_Value', axis=1)\n",
        "        else:\n",
        "            feature_importance_df = feature_importance_df.sort_values(importance_type, ascending=False)\n",
        "\n",
        "        return feature_importance_df\n",
        "\n",
        "    def _save_feature_importance(self, feature_importance_df: pd.DataFrame,\n",
        "                                 model, model_name: str) -> None:\n",
        "        \"\"\"Save feature importance results.\"\"\"\n",
        "        filename = f'{model_name.lower().replace(\" \", \"_\")}_feature_importance.csv'\n",
        "\n",
        "        if hasattr(model, 'intercept_'):\n",
        "            intercept_row = pd.DataFrame({\n",
        "                'Feature': ['[INTERCEPT]'],\n",
        "                feature_importance_df.columns[1]: [model.intercept_]\n",
        "            })\n",
        "            full_df = pd.concat([intercept_row, feature_importance_df], ignore_index=True)\n",
        "            Utilities.save_dataframe(full_df, filename)\n",
        "            self.results['intercept'] = model.intercept_\n",
        "        else:\n",
        "            Utilities.save_dataframe(feature_importance_df, filename)\n",
        "\n",
        "        self.results['feature_importance'] = feature_importance_df\n",
        "        self.results['best_model_name'] = model_name"
      ],
      "metadata": {
        "id": "ABgCx1c4VI_7"
      },
      "id": "ABgCx1c4VI_7",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "033f5f9c",
        "outputId": "a0200220-e5cd-4817-c677-5faaf6899a46"
      },
      "source": [
        "# Run ML models\n",
        "if data_processor.clean_data is not None:\n",
        "    ml_model_builder = LBGMLModelBuilder(data_processor.clean_data, data_processor.target_variable)\n",
        "    ml_model_builder.build_ml_models()\n",
        "    ml_model_builder.evaluate_ml_models()\n",
        "else:\n",
        "    print(\"Data not loaded or cleaned. Please run previous steps.\")"
      ],
      "id": "033f5f9c",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================== MACHINE LEARNING ==================================================\n",
            "Training data shape: (22507, 30), Test data shape: (5627, 30)\n",
            "\n",
            "Training Linear Regression...\n",
            "  RMSE: ¬£1,534.81, MAE: ¬£970.17, R¬≤: 0.9765\n",
            "\n",
            "Training Ridge...\n",
            "  RMSE: ¬£1,535.03, MAE: ¬£970.32, R¬≤: 0.9765\n",
            "\n",
            "Training Lasso...\n",
            "  RMSE: ¬£1,535.83, MAE: ¬£970.35, R¬≤: 0.9765\n",
            "\n",
            "Training Random Forest...\n",
            "  RMSE: ¬£352.43, MAE: ¬£188.31, R¬≤: 0.9988\n",
            "\n",
            "Training Gradient Boosting...\n",
            "  RMSE: ¬£797.37, MAE: ¬£582.36, R¬≤: 0.9937\n",
            "\n",
            "Training XGBoost...\n",
            "  RMSE: ¬£817.38, MAE: ¬£339.51, R¬≤: 0.9933\n",
            "\n",
            "ML Model Training Complete.\n",
            "================================================== ML MODEL EVALUATION ==================================================\n",
            "\n",
            "Model Comparison Table:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f53e7563200>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_be464\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_be464_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_be464_level0_col1\" class=\"col_heading level0 col1\" >RMSE (¬£)</th>\n",
              "      <th id=\"T_be464_level0_col2\" class=\"col_heading level0 col2\" >MAE (¬£)</th>\n",
              "      <th id=\"T_be464_level0_col3\" class=\"col_heading level0 col3\" >R¬≤</th>\n",
              "      <th id=\"T_be464_level0_col4\" class=\"col_heading level0 col4\" >Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
              "      <td id=\"T_be464_row0_col0\" class=\"data row0 col0\" >Random Forest</td>\n",
              "      <td id=\"T_be464_row0_col1\" class=\"data row0 col1\" >¬£352.43</td>\n",
              "      <td id=\"T_be464_row0_col2\" class=\"data row0 col2\" >¬£188.31</td>\n",
              "      <td id=\"T_be464_row0_col3\" class=\"data row0 col3\" >1.0000</td>\n",
              "      <td id=\"T_be464_row0_col4\" class=\"data row0 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
              "      <td id=\"T_be464_row1_col0\" class=\"data row1 col0\" >Gradient Boosting</td>\n",
              "      <td id=\"T_be464_row1_col1\" class=\"data row1 col1\" >¬£797.37</td>\n",
              "      <td id=\"T_be464_row1_col2\" class=\"data row1 col2\" >¬£582.36</td>\n",
              "      <td id=\"T_be464_row1_col3\" class=\"data row1 col3\" >0.9900</td>\n",
              "      <td id=\"T_be464_row1_col4\" class=\"data row1 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
              "      <td id=\"T_be464_row2_col0\" class=\"data row2 col0\" >XGBoost</td>\n",
              "      <td id=\"T_be464_row2_col1\" class=\"data row2 col1\" >¬£817.38</td>\n",
              "      <td id=\"T_be464_row2_col2\" class=\"data row2 col2\" >¬£339.51</td>\n",
              "      <td id=\"T_be464_row2_col3\" class=\"data row2 col3\" >0.9900</td>\n",
              "      <td id=\"T_be464_row2_col4\" class=\"data row2 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row3\" class=\"row_heading level0 row3\" >0</th>\n",
              "      <td id=\"T_be464_row3_col0\" class=\"data row3 col0\" >Linear Regression</td>\n",
              "      <td id=\"T_be464_row3_col1\" class=\"data row3 col1\" >¬£1,534.81</td>\n",
              "      <td id=\"T_be464_row3_col2\" class=\"data row3 col2\" >¬£970.17</td>\n",
              "      <td id=\"T_be464_row3_col3\" class=\"data row3 col3\" >0.9800</td>\n",
              "      <td id=\"T_be464_row3_col4\" class=\"data row3 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row4\" class=\"row_heading level0 row4\" >1</th>\n",
              "      <td id=\"T_be464_row4_col0\" class=\"data row4 col0\" >Ridge</td>\n",
              "      <td id=\"T_be464_row4_col1\" class=\"data row4 col1\" >¬£1,535.03</td>\n",
              "      <td id=\"T_be464_row4_col2\" class=\"data row4 col2\" >¬£970.32</td>\n",
              "      <td id=\"T_be464_row4_col3\" class=\"data row4 col3\" >0.9800</td>\n",
              "      <td id=\"T_be464_row4_col4\" class=\"data row4 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row5\" class=\"row_heading level0 row5\" >2</th>\n",
              "      <td id=\"T_be464_row5_col0\" class=\"data row5 col0\" >Lasso</td>\n",
              "      <td id=\"T_be464_row5_col1\" class=\"data row5 col1\" >¬£1,535.83</td>\n",
              "      <td id=\"T_be464_row5_col2\" class=\"data row5 col2\" >¬£970.35</td>\n",
              "      <td id=\"T_be464_row5_col3\" class=\"data row5 col3\" >0.9800</td>\n",
              "      <td id=\"T_be464_row5_col4\" class=\"data row5 col4\" >Machine Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row6\" class=\"row_heading level0 row6\" >8</th>\n",
              "      <td id=\"T_be464_row6_col0\" class=\"data row6 col0\" >Two-Part (Logit+OLS)</td>\n",
              "      <td id=\"T_be464_row6_col1\" class=\"data row6 col1\" >¬£3,128.99</td>\n",
              "      <td id=\"T_be464_row6_col2\" class=\"data row6 col2\" >¬£2,024.26</td>\n",
              "      <td id=\"T_be464_row6_col3\" class=\"data row6 col3\" >0.9000</td>\n",
              "      <td id=\"T_be464_row6_col4\" class=\"data row6 col4\" >Two-Part</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_be464_row7_col0\" class=\"data row7 col0\" >OLS Only</td>\n",
              "      <td id=\"T_be464_row7_col1\" class=\"data row7 col1\" >¬£8,555.31</td>\n",
              "      <td id=\"T_be464_row7_col2\" class=\"data row7 col2\" >¬£5,505.69</td>\n",
              "      <td id=\"T_be464_row7_col3\" class=\"data row7 col3\" >0.2700</td>\n",
              "      <td id=\"T_be464_row7_col4\" class=\"data row7 col4\" >OLS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_be464_level0_row8\" class=\"row_heading level0 row8\" >6</th>\n",
              "      <td id=\"T_be464_row8_col0\" class=\"data row8 col0\" >Baseline (Mean)</td>\n",
              "      <td id=\"T_be464_row8_col1\" class=\"data row8 col1\" >¬£10,025.17</td>\n",
              "      <td id=\"T_be464_row8_col2\" class=\"data row8 col2\" >¬£7,739.42</td>\n",
              "      <td id=\"T_be464_row8_col3\" class=\"data row8 col3\" >0.0000</td>\n",
              "      <td id=\"T_be464_row8_col4\" class=\"data row8 col4\" >Baseline</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saved: analysis_outputs/ml_model_comparison.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdda7626"
      },
      "source": [
        "#### **Comment**\n",
        "\n",
        "ML models, especially tree-based ones, strongly outperformed statistical models.\n",
        "\n",
        "| Model                | RMSE (¬£)   | MAE (¬£)    | R¬≤     | Type             |\n",
        "|----------------------|------------|------------|--------|------------------|\n",
        "| Random Forest        | 352.43     | 188.31     | 0.9988 | Machine Learning |\n",
        "| Gradient Boosting    | 797.37     | 582.36     | 0.9937 | Machine Learning |\n",
        "| XGBoost              | 817.38     | 339.51     | 0.9933 | Machine Learning |\n",
        "| Linear Regression    | 1,534.81   | 970.17     | 0.9765 | Machine Learning |\n",
        "| Ridge                | 1,535.03   | 970.32     | 0.9765 | Machine Learning |\n",
        "| Lasso                | 1,535.83   | 970.35     | 0.9765 | Machine Learning |\n",
        "| Two-Part (Logit+OLS) | 3,128.99   | 2,024.26   | 0.9026 | Two-Part         |\n",
        "| OLS Only             | 8,555.31   | 5,505.69   | 0.2717 | OLS              |\n",
        "| Baseline (Mean)      | 10,025.17  | 7,739.42   | 0.0000 | Baseline         |\n",
        "\n",
        "Random Forest was the best ML model."
      ],
      "id": "cdda7626"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4: Balancing Performance with Ethical Concerns**\n",
        "Sub-tasks\n",
        "- Identify and analyze ethical concerns related to predictor variables\n",
        "- Explore and potentially implement fairness mitigation techniques\n",
        "- Discuss balancing performance against ethics\n"
      ],
      "metadata": {
        "id": "HRECwzrw0jnr"
      },
      "id": "HRECwzrw0jnr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "tPVk6NVC0jf3"
      },
      "id": "tPVk6NVC0jf3"
    },
    {
      "cell_type": "code",
      "source": [
        "class FairlearnEthicsAnalyzer:\n",
        "    \"\"\"Ethics analyzer using fairlearn for ML model fairness assessment.\"\"\"\n",
        "\n",
        "    PROTECTED_ATTRS = ['gender', 'ethnicity_group', 'age_group']\n",
        "\n",
        "    def __init__(self, data, data_dictionary_path=None):\n",
        "        \"\"\"\n",
        "        Initialize ethics analyzer.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame with UNENCODED protected attributes\n",
        "            data_dictionary_path: Path to data dictionary file\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.data_dict_path = data_dictionary_path\n",
        "        self.results = {}\n",
        "\n",
        "    def identify_sensitive_variables(self):\n",
        "        \"\"\"Identify sensitive variables using utility function.\"\"\"\n",
        "        return Utilities.identify_sensitive_variables(self.data, self.data_dict_path)\n",
        "\n",
        "    def calculate_fairness_metrics(self, y_true, y_pred, sensitive_features):\n",
        "        \"\"\"Calculate fairness metrics using fairlearn.\"\"\"\n",
        "        fairness = {}\n",
        "        sensitive_features = sensitive_features.reset_index(drop=True)\n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "        for attr in self.PROTECTED_ATTRS:\n",
        "            if attr not in sensitive_features.columns:\n",
        "                continue\n",
        "\n",
        "            sf = sensitive_features[attr].astype(str)\n",
        "            mask = (sf != 'nan') & (sf != 'None') & (sf != '')\n",
        "\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                mf = MetricFrame(\n",
        "                    metrics={\n",
        "                        'mae': mean_absolute_error,\n",
        "                        'mse': mean_squared_error,\n",
        "                        'r2': r2_score\n",
        "                    },\n",
        "                    y_true=y_true[mask],\n",
        "                    y_pred=y_pred[mask],\n",
        "                    sensitive_features=sf.values[mask]\n",
        "                )\n",
        "\n",
        "                fairness[attr] = {\n",
        "                    'by_group': mf.by_group.to_dict(),\n",
        "                    'overall': mf.overall.to_dict(),\n",
        "                    'difference': mf.difference().to_dict(),\n",
        "                    'ratio': mf.ratio().to_dict()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error calculating fairness for {attr}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return fairness\n",
        "\n",
        "    def analyse_model(self, model_name, y_true, y_pred, test_data):\n",
        "        \"\"\"Analyze fairness for a single model.\"\"\"\n",
        "        test_data = test_data.reset_index(drop=True)\n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "        if len(y_true) != len(y_pred):\n",
        "            return {'error': f'Length mismatch: y_true={len(y_true)}, y_pred={len(y_pred)}'}\n",
        "\n",
        "        if len(y_true) != len(test_data):\n",
        "            return {'error': f'Length mismatch: y_true={len(y_true)}, test_data={len(test_data)}'}\n",
        "\n",
        "        available = [col for col in self.PROTECTED_ATTRS if col in test_data.columns]\n",
        "\n",
        "        if len(available) == 0:\n",
        "            return {'error': 'No protected attributes found in test data'}\n",
        "\n",
        "        sensitive_features = test_data[available].copy().reset_index(drop=True)\n",
        "        fairness = self.calculate_fairness_metrics(y_true, y_pred, sensitive_features)\n",
        "\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'overall_performance': {\n",
        "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "                'mae': mean_absolute_error(y_true, y_pred),\n",
        "                'r2': r2_score(y_true, y_pred)\n",
        "            },\n",
        "            'fairness_by_attribute': fairness\n",
        "        }\n",
        "\n",
        "    def run_model_analysis(self, models_dict, y_test, test_data):\n",
        "        \"\"\"Run complete ethics analysis.\"\"\"\n",
        "        self.results = {\n",
        "            'sensitive_variables': self.identify_sensitive_variables(),\n",
        "            'model_fairness': {}\n",
        "        }\n",
        "\n",
        "        for model_name, model_info in models_dict.items():\n",
        "            y_pred = model_info.get('predictions')\n",
        "            if y_pred is not None:\n",
        "                analysis = self.analyse_model(model_name, y_test, y_pred, test_data)\n",
        "                self.results['model_fairness'][model_name] = analysis\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print readable fairness report using utility function.\"\"\"\n",
        "        Utilities.print_fairness_report(self.results)"
      ],
      "metadata": {
        "id": "ag73AK1ulAzY"
      },
      "id": "ag73AK1ulAzY",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FairlearnEthicsAnalyzer:\n",
        "    \"\"\"Ethics analyzer using fairlearn for ML model fairness assessment.\"\"\"\n",
        "\n",
        "    PROTECTED_ATTRS = ['gender', 'ethnicity_group', 'age_group']\n",
        "\n",
        "    def __init__(self, data, data_dictionary_path=None):\n",
        "        \"\"\"\n",
        "        Initialize ethics analyzer.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame with UNENCODED protected attributes (gender, ethnicity_group, age_group)\n",
        "                       Should contain original categorical labels for interpretability\n",
        "            data_dictionary_path: Path to data dictionary file\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.data_dict_path = Path(data_dictionary_path) if data_dictionary_path else None\n",
        "        self.results = {}\n",
        "\n",
        "    def identify_sensitive_variables(self):\n",
        "        \"\"\"Identify sensitive variables from data dictionary and columns.\"\"\"\n",
        "        sensitive = {\n",
        "            'protected_characteristics': [],\n",
        "            'proxy_variables': [],\n",
        "            'socioeconomic_variables': [],\n",
        "            'other_sensitive': []\n",
        "        }\n",
        "\n",
        "        # Load data dictionary if available\n",
        "        if self.data_dict_path is not None:\n",
        "            if self.data_dict_path.exists():\n",
        "                try:\n",
        "                    dd = pd.read_excel(self.data_dict_path)\n",
        "                    var_col = dd.columns[0]\n",
        "                    desc_col = dd.columns[1] if len(dd.columns) > 1 else None\n",
        "\n",
        "                    if desc_col is not None:\n",
        "                        for _, row in dd.iterrows():\n",
        "                            var = row[var_col]\n",
        "                            desc = str(row[desc_col]).lower()\n",
        "\n",
        "                            if any(k in var.lower() or k in desc for k in ['gender', 'ethnicity', 'age', 'race', 'sex']):\n",
        "                                sensitive['protected_characteristics'].append(var)\n",
        "                            elif any(k in var.lower() or k in desc for k in ['location', 'region', 'zip', 'postcode', 'address']):\n",
        "                                sensitive['proxy_variables'].append(var)\n",
        "                            elif any(k in var.lower() or k in desc for k in ['income', 'salary', 'wealth', 'savings', 'credit', 'debt']):\n",
        "                                sensitive['socioeconomic_variables'].append(var)\n",
        "                            elif any(k in var.lower() or k in desc for k in ['health', 'medical', 'disability', 'religion', 'political']):\n",
        "                                sensitive['other_sensitive'].append(var)\n",
        "                except Exception as e:\n",
        "                    warnings.warn(f\"Error loading data dictionary: {e}\")\n",
        "\n",
        "        # Check actual columns\n",
        "        for col in self.data.columns:\n",
        "            col_lower = col.lower()\n",
        "\n",
        "            if any(t in col_lower for t in ['gender', 'ethnicity', 'age_group', 'race', 'sex']):\n",
        "                if col not in sensitive['protected_characteristics']:\n",
        "                    sensitive['protected_characteristics'].append(col)\n",
        "            elif any(t in col_lower for t in ['region', 'location', 'postcode', 'zip', 'area']):\n",
        "                if col not in sensitive['proxy_variables']:\n",
        "                    sensitive['proxy_variables'].append(col)\n",
        "            elif any(t in col_lower for t in ['income', 'salary', 'savings', 'credit', 'wealth', 'debt']):\n",
        "                if col not in sensitive['socioeconomic_variables']:\n",
        "                    sensitive['socioeconomic_variables'].append(col)\n",
        "            elif any(t in col_lower for t in ['health', 'medical', 'disability', 'religion', 'political']):\n",
        "                if col not in sensitive['other_sensitive']:\n",
        "                    sensitive['other_sensitive'].append(col)\n",
        "\n",
        "        return sensitive\n",
        "\n",
        "    def calculate_fairness_metrics(self, y_true, y_pred, sensitive_features):\n",
        "        \"\"\"\n",
        "        Calculate fairness metrics using fairlearn.\n",
        "\n",
        "        Args:\n",
        "            y_true: True target values\n",
        "            y_pred: Predicted values\n",
        "            sensitive_features: DataFrame with unencoded protected attributes\n",
        "        \"\"\"\n",
        "        fairness = {}\n",
        "        sensitive_features = sensitive_features.reset_index(drop=True)\n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "        for attr in self.PROTECTED_ATTRS:\n",
        "            if attr not in sensitive_features.columns:\n",
        "                continue\n",
        "\n",
        "            # Clean data - keep original categorical values\n",
        "            sf = sensitive_features[attr].astype(str)\n",
        "            mask = (sf != 'nan') & (sf != 'None') & (sf != '')\n",
        "\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Calculate metrics by group\n",
        "                mf = MetricFrame(\n",
        "                    metrics={\n",
        "                        'mae': mean_absolute_error,\n",
        "                        'mse': mean_squared_error,\n",
        "                        'r2': r2_score\n",
        "                    },\n",
        "                    y_true=y_true[mask],\n",
        "                    y_pred=y_pred[mask],\n",
        "                    sensitive_features=sf.values[mask]\n",
        "                )\n",
        "\n",
        "                fairness[attr] = {\n",
        "                    'by_group': mf.by_group.to_dict(),\n",
        "                    'overall': mf.overall.to_dict(),\n",
        "                    'difference': mf.difference().to_dict(),\n",
        "                    'ratio': mf.ratio().to_dict()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error calculating fairness for {attr}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return fairness\n",
        "\n",
        "    def analyse_model(self, model_name, y_true, y_pred, test_data):\n",
        "        \"\"\"\n",
        "        Analyze fairness for a single model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model\n",
        "            y_true: True target values\n",
        "            y_pred: Predicted values\n",
        "            test_data: Test dataset with unencoded protected attributes\n",
        "        \"\"\"\n",
        "        test_data = test_data.reset_index(drop=True)\n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "        # Verify lengths match\n",
        "        if len(y_true) != len(y_pred):\n",
        "            return {'error': f'Length mismatch: y_true={len(y_true)}, y_pred={len(y_pred)}'}\n",
        "\n",
        "        if len(y_true) != len(test_data):\n",
        "            return {'error': f'Length mismatch: y_true={len(y_true)}, test_data={len(test_data)}'}\n",
        "\n",
        "        # Get protected attributes from test data\n",
        "        available = [col for col in self.PROTECTED_ATTRS if col in test_data.columns]\n",
        "\n",
        "        if len(available) == 0:\n",
        "            return {'error': 'No protected attributes found in test data'}\n",
        "\n",
        "        sensitive_features = test_data[available].copy().reset_index(drop=True)\n",
        "        fairness = self.calculate_fairness_metrics(y_true, y_pred, sensitive_features)\n",
        "\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'overall_performance': {\n",
        "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "                'mae': mean_absolute_error(y_true, y_pred),\n",
        "                'r2': r2_score(y_true, y_pred)\n",
        "            },\n",
        "            'fairness_by_attribute': fairness\n",
        "        }\n",
        "\n",
        "    def run_model_analysis(self, models_dict, y_test, test_data):\n",
        "        \"\"\"\n",
        "        Run complete ethics analysis.\n",
        "\n",
        "        Args:\n",
        "            models_dict: Dictionary of models with predictions\n",
        "            y_test: Test target values\n",
        "            test_data: Test dataset with unencoded protected attributes\n",
        "        \"\"\"\n",
        "        self.results = {\n",
        "            'sensitive_variables': self.identify_sensitive_variables(),\n",
        "            'model_fairness': {}\n",
        "        }\n",
        "\n",
        "        for model_name, model_info in models_dict.items():\n",
        "            y_pred = model_info.get('predictions')\n",
        "            if y_pred is not None:\n",
        "                analysis = self.analyse_model(model_name, y_test, y_pred, test_data)\n",
        "                self.results['model_fairness'][model_name] = analysis\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print readable fairness report.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ETHICS & FAIRNESS ANALYSIS REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Sensitive Variables\n",
        "        print(\"\\n1. SENSITIVE VARIABLES IDENTIFIED:\")\n",
        "        sensitive = self.results.get('sensitive_variables', {})\n",
        "        total = sum(len(v) for v in sensitive.values())\n",
        "\n",
        "        if total > 0:\n",
        "            for category, vars_list in sensitive.items():\n",
        "                if len(vars_list) > 0:\n",
        "                    print(f\"\\n   {category.replace('_', ' ').title()}:\")\n",
        "                    for var in vars_list:\n",
        "                        print(f\"      ‚Ä¢ {var}\")\n",
        "        else:\n",
        "            print(\"   ‚úì No sensitive variables detected\")\n",
        "\n",
        "        # Model Fairness\n",
        "        print(\"\\n2. MODEL FAIRNESS METRICS:\")\n",
        "        model_fairness = self.results.get('model_fairness', {})\n",
        "\n",
        "        if len(model_fairness) == 0:\n",
        "            print(\"   ‚ö† No model fairness metrics available\")\n",
        "\n",
        "        for model_name, metrics in model_fairness.items():\n",
        "            print(f\"\\n   Model: {model_name}\")\n",
        "            print(\"   \" + \"-\" * 70)\n",
        "\n",
        "            if 'error' in metrics:\n",
        "                print(f\"      ‚ö† Error: {metrics['error']}\")\n",
        "                continue\n",
        "\n",
        "            # Overall performance\n",
        "            perf = metrics.get('overall_performance', {})\n",
        "            print(f\"      Overall Performance:\")\n",
        "            print(f\"         RMSE: {perf.get('rmse', 0):.4f}\")\n",
        "            print(f\"         MAE:  {perf.get('mae', 0):.4f}\")\n",
        "            print(f\"         R¬≤:   {perf.get('r2', 0):.4f}\")\n",
        "\n",
        "            # Fairness by attribute\n",
        "            fairness_by_attr = metrics.get('fairness_by_attribute', {})\n",
        "\n",
        "            if len(fairness_by_attr) == 0:\n",
        "                print(f\"\\n      ‚ö† No fairness metrics calculated\")\n",
        "                continue\n",
        "\n",
        "            for attr, attr_metrics in fairness_by_attr.items():\n",
        "                print(f\"\\n      Protected Attribute: {attr}\")\n",
        "\n",
        "                by_group = attr_metrics.get('by_group', {}).get('mae', {})\n",
        "                if by_group:\n",
        "                    print(f\"         MAE by Group:\")\n",
        "                    for group, val in sorted(by_group.items()):\n",
        "                        print(f\"            {group}: {val:.4f}\")\n",
        "\n",
        "                diff = attr_metrics.get('difference', {}).get('mae', 0)\n",
        "                ratio = attr_metrics.get('ratio', {}).get('mae', 0)\n",
        "\n",
        "                print(f\"         Fairness Metrics:\")\n",
        "                print(f\"            Difference (max - min): {diff:.4f}\")\n",
        "                print(f\"            Ratio (max / min):      {ratio:.4f}\")\n",
        "\n",
        "                # Flag potential issues\n",
        "                if ratio > 1.25:\n",
        "                    print(f\"            ‚ö† WARNING: Ratio exceeds 1.25 threshold\")\n",
        "\n",
        "        # Recommendations\n",
        "        print(\"\\n3. RECOMMENDATIONS:\")\n",
        "        if total > 0:\n",
        "            print(\"   ‚ö† Sensitive variables detected in dataset\")\n",
        "            print(\"   ‚Ä¢ Review whether protected attributes should be model features\")\n",
        "            print(\"   ‚Ä¢ Monitor for proxy discrimination through correlated features\")\n",
        "            print(\"   ‚Ä¢ Consider fairness constraints during model training\")\n",
        "\n",
        "        print(\"   ‚Ä¢ Continuously monitor fairness metrics in production\")\n",
        "        print(\"   ‚Ä¢ Document model limitations and potential biases\")\n",
        "        print(\"   ‚Ä¢ Establish fairness thresholds (e.g., ratio < 1.25, difference < 0.1)\")\n",
        "        print(\"   ‚Ä¢ Conduct regular fairness audits across demographic groups\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "def run_ethics_analysis(data_processor, ml_model_builder, dictionary_path):\n",
        "    \"\"\"\n",
        "    Run ethics analysis with automatic data alignment.\n",
        "\n",
        "    IMPORTANT: Assumes data_processor.clean_data contains UNENCODED protected attributes\n",
        "    (gender, ethnicity_group, age_group) for interpretable fairness reporting.\n",
        "\n",
        "    Args:\n",
        "        data_processor: DataProcessor object with clean_data containing unencoded protected attrs\n",
        "        ml_model_builder: MLModelBuilder object with results containing predictions\n",
        "        dictionary_path: Path to data dictionary file\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RUNNING ETHICS ANALYSIS\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    y_test = ml_model_builder.results.get('y_test')\n",
        "    ml_models = ml_model_builder.results.get('ml_models', {})\n",
        "\n",
        "    if y_test is None:\n",
        "        print(\"‚ö† Test data not available\")\n",
        "        return None\n",
        "\n",
        "    if len(ml_models) == 0:\n",
        "        print(\"‚ö† ML models not available\")\n",
        "        return None\n",
        "\n",
        "    processed_data = data_processor.clean_data.copy()\n",
        "    test_data = None\n",
        "    if hasattr(y_test, 'index'):\n",
        "        try:\n",
        "            test_data = processed_data.loc[y_test.index].copy()\n",
        "            print(f\"‚úì Test data extracted using index alignment (shape: {test_data.shape})\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Could not use index alignment: {e}\")\n",
        "\n",
        "\n",
        "    # Verify test_data contains protected attributes\n",
        "    available_attrs = [col for col in FairlearnEthicsAnalyzer.PROTECTED_ATTRS if col in test_data.columns]\n",
        "    if len(available_attrs) == 0:\n",
        "        print(f\"‚ö† WARNING: No protected attributes found in test data\")\n",
        "        print(f\"   Available columns: {list(test_data.columns)}\")\n",
        "        print(f\"   Expected one of: {FairlearnEthicsAnalyzer.PROTECTED_ATTRS}\")\n",
        "    else:\n",
        "        print(f\"‚úì Protected attributes found: {available_attrs}\")\n",
        "\n",
        "    # Prepare models dictionary\n",
        "    models_dict = {}\n",
        "    for model_name, model_info in ml_models.items():\n",
        "        predictions = model_info.get('predictions')\n",
        "        if predictions is not None:\n",
        "            models_dict[model_name] = {'predictions': predictions}\n",
        "            print(f\"‚úì Model '{model_name}' ready for analysis (predictions shape: {np.array(predictions).shape})\")\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = FairlearnEthicsAnalyzer(\n",
        "        data=processed_data,\n",
        "        data_dictionary_path=dictionary_path\n",
        "    )\n",
        "\n",
        "    # Run analysis\n",
        "    try:\n",
        "        results = analyzer.run_model_analysis(models_dict, y_test, test_data)\n",
        "        analyzer.print_report()\n",
        "\n",
        "        # Save results\n",
        "        output_dir = Path(\"analysis_outputs\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        output_path = output_dir / 'ethics_analysis.json'\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "        return analyzer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "analyzer = run_ethics_analysis(data_processor, ml_model_builder, dictionary_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KsIiPEHPZya",
        "outputId": "291b3831-6963-4bb5-a7f5-ba1a92201ce7"
      },
      "id": "2KsIiPEHPZya",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "RUNNING ETHICS ANALYSIS\n",
            "==================================================\n",
            "\n",
            "‚úì Test data extracted using index alignment (shape: (5627, 20))\n",
            "‚úì Protected attributes found: ['gender', 'ethnicity_group', 'age_group']\n",
            "‚úì Model 'Linear Regression' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Ridge' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Lasso' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Random Forest' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Gradient Boosting' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'XGBoost' ready for analysis (predictions shape: (5627,))\n",
            "\n",
            "================================================================================\n",
            "ETHICS & FAIRNESS ANALYSIS REPORT\n",
            "================================================================================\n",
            "\n",
            "1. SENSITIVE VARIABLES IDENTIFIED:\n",
            "\n",
            "   Protected Characteristics:\n",
            "      ‚Ä¢ age\n",
            "      ‚Ä¢ gender\n",
            "      ‚Ä¢ ethnicity_group\n",
            "      ‚Ä¢ housing_spend\n",
            "      ‚Ä¢ age_group\n",
            "\n",
            "   Proxy Variables:\n",
            "      ‚Ä¢ geo_region\n",
            "\n",
            "   Socioeconomic Variables:\n",
            "      ‚Ä¢ net_salary\n",
            "      ‚Ä¢ other_income\n",
            "      ‚Ä¢ observed_income\n",
            "      ‚Ä¢ observed_surplus\n",
            "      ‚Ä¢ credit_score\n",
            "      ‚Ä¢ savings_bal_lbg\n",
            "      ‚Ä¢ annual_net_savings_lbg\n",
            "      ‚Ä¢ savings_rate\n",
            "      ‚Ä¢ discretionary_income\n",
            "\n",
            "2. MODEL FAIRNESS METRICS:\n",
            "\n",
            "   Model: Linear Regression\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1534.8054\n",
            "         MAE:  970.1749\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 884.0820\n",
            "            F: 968.8521\n",
            "            M: 971.3974\n",
            "            Other: 1037.9115\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.8295\n",
            "            Ratio (max / min):      0.8518\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1164.7259\n",
            "            Other: 941.2094\n",
            "            South Asian: 1302.5291\n",
            "            White British: 925.7565\n",
            "            White Other: 1034.8765\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 376.7726\n",
            "            Ratio (max / min):      0.7107\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 730.2301\n",
            "            30-39: 780.0580\n",
            "            40-49: 1017.3349\n",
            "            50-59: 1372.2588\n",
            "            60+: 1048.3391\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 642.0287\n",
            "            Ratio (max / min):      0.5321\n",
            "\n",
            "   Model: Ridge\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1535.0333\n",
            "         MAE:  970.3167\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 884.3096\n",
            "            F: 968.9561\n",
            "            M: 971.5756\n",
            "            Other: 1038.0210\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.7114\n",
            "            Ratio (max / min):      0.8519\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1164.6555\n",
            "            Other: 941.3806\n",
            "            South Asian: 1302.6039\n",
            "            White British: 925.9163\n",
            "            White Other: 1034.9151\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 376.6875\n",
            "            Ratio (max / min):      0.7108\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 729.9107\n",
            "            30-39: 780.3236\n",
            "            40-49: 1017.6428\n",
            "            50-59: 1372.7135\n",
            "            60+: 1048.4944\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 642.8028\n",
            "            Ratio (max / min):      0.5317\n",
            "\n",
            "   Model: Lasso\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1535.8250\n",
            "         MAE:  970.3475\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 887.5840\n",
            "            F: 969.1284\n",
            "            M: 971.4319\n",
            "            Other: 1036.4402\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 148.8561\n",
            "            Ratio (max / min):      0.8564\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1163.5298\n",
            "            Other: 942.3052\n",
            "            South Asian: 1304.0022\n",
            "            White British: 925.7576\n",
            "            White Other: 1036.0283\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 378.2446\n",
            "            Ratio (max / min):      0.7099\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 727.1758\n",
            "            30-39: 780.3672\n",
            "            40-49: 1017.8569\n",
            "            50-59: 1375.4637\n",
            "            60+: 1049.4885\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 648.2879\n",
            "            Ratio (max / min):      0.5287\n",
            "\n",
            "   Model: Random Forest\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 352.4300\n",
            "         MAE:  188.3129\n",
            "         R¬≤:   0.9988\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 140.5677\n",
            "            F: 184.6221\n",
            "            M: 192.0507\n",
            "            Other: 222.9348\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 82.3671\n",
            "            Ratio (max / min):      0.6305\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 278.6049\n",
            "            Other: 209.3137\n",
            "            South Asian: 270.2210\n",
            "            White British: 174.2602\n",
            "            White Other: 234.8214\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 104.3447\n",
            "            Ratio (max / min):      0.6255\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 116.1861\n",
            "            30-39: 179.5925\n",
            "            40-49: 236.1545\n",
            "            50-59: 233.1224\n",
            "            60+: 205.9547\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 119.9684\n",
            "            Ratio (max / min):      0.4920\n",
            "\n",
            "   Model: Gradient Boosting\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 797.3748\n",
            "         MAE:  582.3643\n",
            "         R¬≤:   0.9937\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 525.7035\n",
            "            F: 576.1361\n",
            "            M: 589.6099\n",
            "            Other: 598.9641\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 73.2606\n",
            "            Ratio (max / min):      0.8777\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 689.8948\n",
            "            Other: 603.0980\n",
            "            South Asian: 691.4807\n",
            "            White British: 567.8282\n",
            "            White Other: 576.1057\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 123.6525\n",
            "            Ratio (max / min):      0.8212\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 485.0155\n",
            "            30-39: 634.5544\n",
            "            40-49: 638.9553\n",
            "            50-59: 588.4336\n",
            "            60+: 603.2722\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.9398\n",
            "            Ratio (max / min):      0.7591\n",
            "\n",
            "   Model: XGBoost\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 817.3833\n",
            "         MAE:  339.5135\n",
            "         R¬≤:   0.9933\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 256.3123\n",
            "            F: 331.6085\n",
            "            M: 348.8926\n",
            "            Other: 364.5350\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 108.2227\n",
            "            Ratio (max / min):      0.7031\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 430.3597\n",
            "            Other: 282.7767\n",
            "            South Asian: 358.1032\n",
            "            White British: 336.9786\n",
            "            White Other: 318.6984\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 147.5830\n",
            "            Ratio (max / min):      0.6571\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 439.5609\n",
            "            30-39: 289.6965\n",
            "            40-49: 318.3280\n",
            "            50-59: 317.4470\n",
            "            60+: 287.2107\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 152.3502\n",
            "            Ratio (max / min):      0.6534\n",
            "\n",
            "3. RECOMMENDATIONS:\n",
            "   ‚ö† Sensitive variables detected in dataset\n",
            "   ‚Ä¢ Review whether protected attributes should be model features\n",
            "   ‚Ä¢ Monitor for proxy discrimination through correlated features\n",
            "   ‚Ä¢ Consider fairness constraints during model training\n",
            "   ‚Ä¢ Continuously monitor fairness metrics in production\n",
            "   ‚Ä¢ Document model limitations and potential biases\n",
            "   ‚Ä¢ Establish fairness thresholds (e.g., ratio < 1.25, difference < 0.1)\n",
            "   ‚Ä¢ Conduct regular fairness audits across demographic groups\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üíæ Results saved to: analysis_outputs/ethics_analysis.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDc5ucH0ODkF",
        "outputId": "ad735e23-6e0b-48ba-85f2-623de92bc6d8"
      },
      "id": "iDc5ucH0ODkF",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "RUNNING ETHICS ANALYSIS\n",
            "==================================================\n",
            "\n",
            "‚úì Test data extracted using index alignment (shape: (5627, 20))\n",
            "‚úì Protected attributes found: ['gender', 'ethnicity_group', 'age_group']\n",
            "‚úì Model 'Linear Regression' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Ridge' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Lasso' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Random Forest' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'Gradient Boosting' ready for analysis (predictions shape: (5627,))\n",
            "‚úì Model 'XGBoost' ready for analysis (predictions shape: (5627,))\n",
            "\n",
            "================================================================================\n",
            "ETHICS & FAIRNESS ANALYSIS REPORT\n",
            "================================================================================\n",
            "\n",
            "1. SENSITIVE VARIABLES IDENTIFIED:\n",
            "\n",
            "   Protected Characteristics:\n",
            "      ‚Ä¢ age\n",
            "      ‚Ä¢ gender\n",
            "      ‚Ä¢ ethnicity_group\n",
            "      ‚Ä¢ housing_spend\n",
            "      ‚Ä¢ age_group\n",
            "\n",
            "   Proxy Variables:\n",
            "      ‚Ä¢ geo_region\n",
            "\n",
            "   Socioeconomic Variables:\n",
            "      ‚Ä¢ net_salary\n",
            "      ‚Ä¢ other_income\n",
            "      ‚Ä¢ observed_income\n",
            "      ‚Ä¢ observed_surplus\n",
            "      ‚Ä¢ credit_score\n",
            "      ‚Ä¢ savings_bal_lbg\n",
            "      ‚Ä¢ annual_net_savings_lbg\n",
            "      ‚Ä¢ savings_rate\n",
            "      ‚Ä¢ discretionary_income\n",
            "\n",
            "2. MODEL FAIRNESS METRICS:\n",
            "\n",
            "   Model: Linear Regression\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1534.8054\n",
            "         MAE:  970.1749\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 884.0820\n",
            "            F: 968.8521\n",
            "            M: 971.3974\n",
            "            Other: 1037.9115\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.8295\n",
            "            Ratio (max / min):      0.8518\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1164.7259\n",
            "            Other: 941.2094\n",
            "            South Asian: 1302.5291\n",
            "            White British: 925.7565\n",
            "            White Other: 1034.8765\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 376.7726\n",
            "            Ratio (max / min):      0.7107\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 730.2301\n",
            "            30-39: 780.0580\n",
            "            40-49: 1017.3349\n",
            "            50-59: 1372.2588\n",
            "            60+: 1048.3391\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 642.0287\n",
            "            Ratio (max / min):      0.5321\n",
            "\n",
            "   Model: Ridge\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1535.0333\n",
            "         MAE:  970.3167\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 884.3096\n",
            "            F: 968.9561\n",
            "            M: 971.5756\n",
            "            Other: 1038.0210\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.7114\n",
            "            Ratio (max / min):      0.8519\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1164.6555\n",
            "            Other: 941.3806\n",
            "            South Asian: 1302.6039\n",
            "            White British: 925.9163\n",
            "            White Other: 1034.9151\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 376.6875\n",
            "            Ratio (max / min):      0.7108\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 729.9107\n",
            "            30-39: 780.3236\n",
            "            40-49: 1017.6428\n",
            "            50-59: 1372.7135\n",
            "            60+: 1048.4944\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 642.8028\n",
            "            Ratio (max / min):      0.5317\n",
            "\n",
            "   Model: Lasso\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 1535.8250\n",
            "         MAE:  970.3475\n",
            "         R¬≤:   0.9765\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 887.5840\n",
            "            F: 969.1284\n",
            "            M: 971.4319\n",
            "            Other: 1036.4402\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 148.8561\n",
            "            Ratio (max / min):      0.8564\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 1163.5298\n",
            "            Other: 942.3052\n",
            "            South Asian: 1304.0022\n",
            "            White British: 925.7576\n",
            "            White Other: 1036.0283\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 378.2446\n",
            "            Ratio (max / min):      0.7099\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 727.1758\n",
            "            30-39: 780.3672\n",
            "            40-49: 1017.8569\n",
            "            50-59: 1375.4637\n",
            "            60+: 1049.4885\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 648.2879\n",
            "            Ratio (max / min):      0.5287\n",
            "\n",
            "   Model: Random Forest\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 352.4300\n",
            "         MAE:  188.3129\n",
            "         R¬≤:   0.9988\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 140.5677\n",
            "            F: 184.6221\n",
            "            M: 192.0507\n",
            "            Other: 222.9348\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 82.3671\n",
            "            Ratio (max / min):      0.6305\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 278.6049\n",
            "            Other: 209.3137\n",
            "            South Asian: 270.2210\n",
            "            White British: 174.2602\n",
            "            White Other: 234.8214\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 104.3447\n",
            "            Ratio (max / min):      0.6255\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 116.1861\n",
            "            30-39: 179.5925\n",
            "            40-49: 236.1545\n",
            "            50-59: 233.1224\n",
            "            60+: 205.9547\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 119.9684\n",
            "            Ratio (max / min):      0.4920\n",
            "\n",
            "   Model: Gradient Boosting\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 797.3748\n",
            "         MAE:  582.3643\n",
            "         R¬≤:   0.9937\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 525.7035\n",
            "            F: 576.1361\n",
            "            M: 589.6099\n",
            "            Other: 598.9641\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 73.2606\n",
            "            Ratio (max / min):      0.8777\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 689.8948\n",
            "            Other: 603.0980\n",
            "            South Asian: 691.4807\n",
            "            White British: 567.8282\n",
            "            White Other: 576.1057\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 123.6525\n",
            "            Ratio (max / min):      0.8212\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 485.0155\n",
            "            30-39: 634.5544\n",
            "            40-49: 638.9553\n",
            "            50-59: 588.4336\n",
            "            60+: 603.2722\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 153.9398\n",
            "            Ratio (max / min):      0.7591\n",
            "\n",
            "   Model: XGBoost\n",
            "   ----------------------------------------------------------------------\n",
            "      Overall Performance:\n",
            "         RMSE: 817.3833\n",
            "         MAE:  339.5135\n",
            "         R¬≤:   0.9933\n",
            "\n",
            "      Protected Attribute: gender\n",
            "         MAE by Group:\n",
            "            9999: 256.3123\n",
            "            F: 331.6085\n",
            "            M: 348.8926\n",
            "            Other: 364.5350\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 108.2227\n",
            "            Ratio (max / min):      0.7031\n",
            "\n",
            "      Protected Attribute: ethnicity_group\n",
            "         MAE by Group:\n",
            "            Afro-Carribbean: 430.3597\n",
            "            Other: 282.7767\n",
            "            South Asian: 358.1032\n",
            "            White British: 336.9786\n",
            "            White Other: 318.6984\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 147.5830\n",
            "            Ratio (max / min):      0.6571\n",
            "\n",
            "      Protected Attribute: age_group\n",
            "         MAE by Group:\n",
            "            18-29: 439.5609\n",
            "            30-39: 289.6965\n",
            "            40-49: 318.3280\n",
            "            50-59: 317.4470\n",
            "            60+: 287.2107\n",
            "         Fairness Metrics:\n",
            "            Difference (max - min): 152.3502\n",
            "            Ratio (max / min):      0.6534\n",
            "\n",
            "3. RECOMMENDATIONS:\n",
            "   ‚ö† Sensitive variables detected in dataset\n",
            "   ‚Ä¢ Review whether protected attributes should be model features\n",
            "   ‚Ä¢ Monitor for proxy discrimination through correlated features\n",
            "   ‚Ä¢ Consider fairness constraints during model training\n",
            "   ‚Ä¢ Continuously monitor fairness metrics in production\n",
            "   ‚Ä¢ Document model limitations and potential biases\n",
            "   ‚Ä¢ Establish fairness thresholds (e.g., ratio < 1.25, difference < 0.1)\n",
            "   ‚Ä¢ Conduct regular fairness audits across demographic groups\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üíæ Results saved to: analysis_outputs/ethics_analysis.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b85f14"
      },
      "source": [
        "#### **Comment**\n",
        "\n",
        "The ethical analysis revealed:\n",
        "\n",
        "*   **Sensitive Variables:** Protected characteristics (age, gender, ethnicity) and proxy variables (geo-region) were identified, raising discrimination concerns.\n",
        "*   **Data Quality:** Negative salaries and under-age entries were found, alongside representation imbalances in gender and ethnicity.\n",
        "*   **Model Fairness:** All models showed performance disparities across protected groups, with XGBoost having the highest.\n",
        "*   **Recommendations:** Removing sensitive variables, exploring fairness techniques, and continuous monitoring are crucial to balance high ML performance with ethical compliance. Ethical concerns should also be an ongoing discussion with domain experts such as lawyers, data protection officers etc."
      ],
      "id": "c9b85f14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 5: Prototype an Application with GenAI**\n",
        "Sub-tasks\n",
        "- Select and parse a sample of the data into a format suitable for an LLM\n",
        "- Use chosen tools (HuggingFace, OpenAI, etc.) to interact with an LLM using the data sample\n",
        "- Explore potential applications or insights gained from the LLM interaction\n"
      ],
      "metadata": {
        "id": "Q_GPI4RVBT0w"
      },
      "id": "Q_GPI4RVBT0w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "QigtmfL4Dfx7"
      },
      "id": "QigtmfL4Dfx7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acd1780"
      },
      "source": [
        "class LBGLLMAnalyzer:\n",
        "    \"\"\"\n",
        "    A class to perform LLM-assisted analysis on LBG customer savings data.\n",
        "    \"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, data_dictionary_path: str, target_variable: str):\n",
        "        self.clean_data = data\n",
        "        self.data_dictionary_path = Path(data_dictionary_path)\n",
        "        self.target_variable = target_variable\n",
        "        self.results = {}\n",
        "\n",
        "    # ... existing methods (llm_analysis, llm_math_assisted_report, etc.) ...\n",
        "\n",
        "    def llm_counterfactual_analysis(self, customer_row: pd.DataFrame, ml_model_builder,\n",
        "                                   scenarios_config: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate 'what-if' counterfactual scenarios for a customer.\n",
        "        Shows how changes to features would affect predicted savings.\n",
        "\n",
        "        Args:\n",
        "            customer_row: Single-row DataFrame with customer data\n",
        "            ml_model_builder: Trained LBGMLModelBuilder instance\n",
        "            scenarios_config: Optional dict defining custom scenarios\n",
        "        \"\"\"\n",
        "\n",
        "        api_key = os.environ.get('OPENAI_API_KEY')\n",
        "        client = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Get best model\n",
        "        best_model_name = min(ml_model_builder.results['ml_models'],\n",
        "                              key=lambda x: ml_model_builder.results['ml_models'][x]['rmse'])\n",
        "        best_model = ml_model_builder.results['ml_models'][best_model_name]['model']\n",
        "\n",
        "        print(f\"\\nüîÆ Running Counterfactual Analysis using: {best_model_name}\")\n",
        "\n",
        "        # Helper function to make prediction\n",
        "        def predict_for_customer(row_data: pd.DataFrame) -> float:\n",
        "            \"\"\"Make prediction for a customer row.\"\"\"\n",
        "            # Prepare data same way as training\n",
        "            X_pred = row_data.copy()\n",
        "\n",
        "            # Remove ID and target columns\n",
        "            id_cols = [col for col in X_pred.columns\n",
        "                      if 'id' in col.lower() or 'unique' in col.lower()]\n",
        "            X_pred = X_pred.drop(columns=id_cols, errors='ignore')\n",
        "\n",
        "            if self.target_variable in X_pred.columns:\n",
        "                X_pred = X_pred.drop(columns=[self.target_variable])\n",
        "\n",
        "            # Ensure same features as training\n",
        "            for col in ml_model_builder.X_clean.columns:\n",
        "                if col not in X_pred.columns:\n",
        "                    X_pred[col] = 0\n",
        "\n",
        "            X_pred = X_pred[ml_model_builder.X_clean.columns]\n",
        "\n",
        "            # Scale features\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(ml_model_builder.X_clean)\n",
        "            X_pred_scaled = scaler.transform(X_pred)\n",
        "\n",
        "            # Predict\n",
        "            prediction = best_model.predict(X_pred_scaled)[0]\n",
        "            return max(prediction, 0)  # Ensure non-negative\n",
        "\n",
        "        # Get baseline prediction\n",
        "        baseline_pred = predict_for_customer(customer_row)\n",
        "        actual_savings = customer_row[self.target_variable].values[0] if self.target_variable in customer_row.columns else None\n",
        "\n",
        "        print(f\"üìä Baseline prediction: ¬£{baseline_pred:,.2f}\")\n",
        "        if actual_savings is not None:\n",
        "            print(f\"üìä Actual savings: ¬£{actual_savings:,.2f}\")\n",
        "\n",
        "        # Define default scenarios if not provided\n",
        "            scenarios_config = {\n",
        "                'Reduce housing spend by 20%': {'housing_spend': 0.8},\n",
        "                'Reduce housing spend by 50%': {'housing_spend': 0.5},\n",
        "                'Eliminate gambling spend': {'gambling_spend': 0.0},\n",
        "                'Reduce childcare spend by 30%': {'childcare_spend': 0.7},\n",
        "                'Increase salary by 10%': {'net_salary': 1.1},\n",
        "                'Increase salary by 20%': {'net_salary': 1.2},\n",
        "                'Reduce all discretionary spending by 25%': {\n",
        "                    'gambling_spend': 0.75,\n",
        "                    'housing_spend': 0.75  # Assuming some housing is discretionary\n",
        "                },\n",
        "                'Increase other income by 50%': {'other_income': 1.5},\n",
        "            }\n",
        "\n",
        "        # Generate scenarios\n",
        "        scenarios = []\n",
        "        for scenario_name, changes in scenarios_config.items():\n",
        "            test_row = customer_row.copy()\n",
        "\n",
        "            # Apply changes\n",
        "            for feature, multiplier in changes.items():\n",
        "                if feature in test_row.columns:\n",
        "                    if multiplier < 1.0 and multiplier > 0:  # Reduction\n",
        "                        test_row[feature] = test_row[feature] * multiplier\n",
        "                    elif multiplier == 0.0:  # Elimination\n",
        "                        test_row[feature] = 0.0\n",
        "                    else:  # Increase\n",
        "                        test_row[feature] = test_row[feature] * multiplier\n",
        "\n",
        "            # Get new prediction\n",
        "            try:\n",
        "                new_pred = predict_for_customer(test_row)\n",
        "                improvement = new_pred - baseline_pred\n",
        "                pct_improvement = (improvement / baseline_pred * 100) if baseline_pred > 0 else 0\n",
        "\n",
        "                scenarios.append({\n",
        "                    'scenario': scenario_name,\n",
        "                    'baseline_pred': baseline_pred,\n",
        "                    'new_pred': new_pred,\n",
        "                    'improvement': improvement,\n",
        "                    'pct_improvement': pct_improvement,\n",
        "                    'changes': changes\n",
        "                })\n",
        "\n",
        "                print(f\"  ‚úì {scenario_name}: ¬£{new_pred:,.2f} ({'+' if improvement >= 0 else ''}¬£{improvement:,.2f}, {pct_improvement:+.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚úó {scenario_name}: Failed ({e})\")\n",
        "\n",
        "        # Sort scenarios by improvement\n",
        "        scenarios.sort(key=lambda x: x['improvement'], reverse=True)\n",
        "\n",
        "        # Extract customer features for context\n",
        "        customer_features = {}\n",
        "        for col in customer_row.columns:\n",
        "            if col not in ['cust_unique_id', self.target_variable]:\n",
        "                value = customer_row[col].values[0]\n",
        "                if pd.notna(value):\n",
        "                    customer_features[col] = float(value) if isinstance(value, (int, float, np.number)) else str(value)\n",
        "\n",
        "        # Build prompt for LLM\n",
        "        prompt_content = f\"\"\"You are analyzing counterfactual 'what-if' scenarios for a customer's savings prediction using a {best_model_name} model.\n",
        "\n",
        "            **Customer Profile:**\n",
        "            \"\"\"\n",
        "        # Add key customer features\n",
        "        key_features = ['age', 'gender', 'net_salary', 'other_income', 'housing_spend',\n",
        "                       'childcare_spend', 'gambling_spend', 'observed_surplus', 'credit_score']\n",
        "        for feature in key_features:\n",
        "            if feature in customer_features:\n",
        "                prompt_content += f\"- {feature}: {customer_features[feature]}\\n\"\n",
        "\n",
        "        prompt_content += f\"\"\"\n",
        "            **Current Situation:**\n",
        "            - Baseline Predicted Savings: ¬£{baseline_pred:,.2f}/year\n",
        "            \"\"\"\n",
        "        if actual_savings is not None:\n",
        "            prompt_content += f\"- Actual Savings: ¬£{actual_savings:,.2f}/year\\n\"\n",
        "\n",
        "        prompt_content += f\"\"\"\n",
        "            **What-If Scenarios (Ranked by Impact):**\n",
        "            \"\"\"\n",
        "\n",
        "        for i, scenario in enumerate(scenarios[:8], 1):  # Top 8 scenarios\n",
        "            sign = '+' if scenario['improvement'] >= 0 else ''\n",
        "            prompt_content += f\"{i}. **{scenario['scenario']}**\\n\"\n",
        "            prompt_content += f\"   - New Prediction: ¬£{scenario['new_pred']:,.2f}\\n\"\n",
        "            prompt_content += f\"   - Change: {sign}¬£{scenario['improvement']:,.2f} ({scenario['pct_improvement']:+.1f}%)\\n\\n\"\n",
        "\n",
        "        prompt_content += \"\"\"\n",
        "            **Analysis Tasks:**\n",
        "            1. **Most Impactful Actions**: Which 2-3 scenarios provide the greatest savings improvement? Why?\n",
        "            2. **Feasibility Assessment**: Which changes are most realistic for this customer to implement?\n",
        "            3. **Personalized Recommendations**: Provide 3-5 specific, actionable recommendations ranked by:\n",
        "              - Expected impact on savings\n",
        "              - Ease of implementation\n",
        "              - Timeline (short-term vs long-term)\n",
        "            4. **Combined Strategy**: Suggest a combination of 2-3 changes that would maximize savings while remaining achievable.\n",
        "\n",
        "            Be specific, practical, and reference actual numbers from the scenarios.\n",
        "            \"\"\"\n",
        "\n",
        "        # Call LLM for analysis\n",
        "        try:\n",
        "            analysis_response = client.chat.completions.create(\n",
        "                model='gpt-4o-mini',\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': 'You are a financial advisor providing personalized, actionable savings advice based on ML model predictions.'},\n",
        "                    {'role': 'user', 'content': prompt_content}\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "                max_tokens=1500\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"OpenAI API call failed: {e}\")\n",
        "\n",
        "        analysis_content = analysis_response.choices[0].message.content\n",
        "\n",
        "        # Build report\n",
        "        report_lines = [\n",
        "            '# Counterfactual Analysis: Personalized Savings Recommendations',\n",
        "            f'\\n## Model: {best_model_name}',\n",
        "            '\\n## Customer Profile\\n'\n",
        "        ]\n",
        "\n",
        "        for feature in key_features:\n",
        "            if feature in customer_features:\n",
        "                report_lines.append(f'- **{feature}**: {customer_features[feature]}')\n",
        "\n",
        "        report_lines.extend([\n",
        "            '\\n## Current Situation\\n',\n",
        "            f'- **Baseline Predicted Savings**: ¬£{baseline_pred:,.2f}/year'\n",
        "        ])\n",
        "\n",
        "        if actual_savings is not None:\n",
        "            error = abs(baseline_pred - actual_savings)\n",
        "            report_lines.append(f'- **Actual Savings**: ¬£{actual_savings:,.2f}/year (Prediction error: ¬£{error:,.2f})')\n",
        "\n",
        "        report_lines.extend(['\\n## What-If Scenarios\\n'])\n",
        "\n",
        "        for i, scenario in enumerate(scenarios, 1):\n",
        "            sign = '+' if scenario['improvement'] >= 0 else ''\n",
        "            impact_emoji = 'üöÄ' if scenario['improvement'] > baseline_pred * 0.1 else 'üìà' if scenario['improvement'] > 0 else 'üìâ'\n",
        "\n",
        "            report_lines.extend([\n",
        "                f'\\n### {i}. {scenario[\"scenario\"]} {impact_emoji}',\n",
        "                f'- **New Prediction**: ¬£{scenario[\"new_pred\"]:,.2f}',\n",
        "                f'- **Change**: {sign}¬£{scenario[\"improvement\"]:,.2f} ({scenario[\"pct_improvement\"]:+.1f}%)',\n",
        "            ])\n",
        "\n",
        "        # Add LLM analysis\n",
        "        report_lines.extend(['\\n## AI-Generated Recommendations\\n', analysis_content])\n",
        "\n",
        "        # Save report\n",
        "        output_dir = Path(\"analysis_outputs\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create a customer-specific filename\n",
        "        customer_id = customer_row['cust_unique_id'].values[0] if 'cust_unique_id' in customer_row.columns else 'unknown'\n",
        "        out_path = output_dir / f'counterfactual_analysis_customer_{customer_id}.md'\n",
        "        out_path.write_text('\\n'.join(report_lines))\n",
        "\n",
        "        print(f\"\\n‚úì Report saved: {out_path}\")\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"AI RECOMMENDATIONS:\")\n",
        "        print('='*80)\n",
        "        print(analysis_content)\n",
        "\n",
        "        # Store results\n",
        "        self.results['counterfactual_scenarios'] = scenarios\n",
        "        self.results['counterfactual_analysis'] = analysis_content\n",
        "\n",
        "        return str(out_path)"
      ],
      "id": "1acd1780",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM Analyzer\n",
        "llm_analyzer = LBGLLMAnalyzer(\n",
        "    data=data_processor.clean_data,\n",
        "    data_dictionary_path=dictionary_path,\n",
        "    target_variable=data_processor.target_variable\n",
        ")\n",
        "\n",
        "def analyze_customers(customer_ids=None, n=3, scenarios=None):\n",
        "    \"\"\"\n",
        "    Run counterfactual analysis on customers.\n",
        "\n",
        "    Args:\n",
        "        customer_ids: Single ID, list of IDs, or None for first n customers\n",
        "        n: Number of customers to analyze if customer_ids is None (default 3)\n",
        "        scenarios: Custom scenarios dict (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select customers (same as before)\n",
        "    if customer_ids is None:\n",
        "        customers = data_processor.clean_data.head(n)\n",
        "        print(f\"üìã Analyzing first {n} customers\")\n",
        "    else:\n",
        "        if not isinstance(customer_ids, list):\n",
        "            customer_ids = [customer_ids]\n",
        "        customers = data_processor.clean_data[\n",
        "            data_processor.clean_data['cust_unique_id'].isin(customer_ids)\n",
        "        ]\n",
        "        if len(customers) == 0:\n",
        "            print(f\"‚ùå No customers found with IDs: {customer_ids}\")\n",
        "            return\n",
        "        print(f\"üìã Analyzing {len(customers)} customer(s)\")\n",
        "\n",
        "    # Analyze each customer\n",
        "    for idx, (i, row) in enumerate(customers.iterrows(), 1):\n",
        "        customer_id = row.get('cust_unique_id', 'N/A')\n",
        "        savings = row[data_processor.target_variable]\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"CUSTOMER {idx}/{len(customers)} - ID: {customer_id} - Savings: ¬£{savings:,.2f}\")\n",
        "        print('='*80)\n",
        "\n",
        "        customer_df = pd.DataFrame([row])\n",
        "\n",
        "        try:\n",
        "            # Pass scenarios if provided\n",
        "            if scenarios:\n",
        "                report_path = llm_analyzer.llm_counterfactual_analysis(\n",
        "                    customer_row=customer_df,\n",
        "                    ml_model_builder=ml_model_builder,\n",
        "                    scenarios_config=scenarios  # Use custom scenarios\n",
        "                )\n",
        "            else:\n",
        "                report_path = llm_analyzer.llm_counterfactual_analysis(\n",
        "                    customer_row=customer_df,\n",
        "                    ml_model_builder=ml_model_builder\n",
        "                )\n",
        "            print(f\"‚úÖ Report saved: {report_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed: {e}\")\n",
        "\n",
        "# First 3 customers (DEFAULT)\n",
        "#analyze_customers(n=2)\n",
        "\n",
        "# Create quick-access functions for different scenario sets\n",
        "def analyze_with_income_focus(customer_ids=None, n=3):\n",
        "    \"\"\"Analyze focusing on income optimization only.\"\"\"\n",
        "    income_scenarios = {\n",
        "        'Negotiate 5% raise': {'net_salary': 1.05},\n",
        "        'Negotiate 10% raise': {'net_salary': 1.10},\n",
        "        'Side hustle ¬£300/month': {'other_income': lambda x: x + 3600},\n",
        "        'Side hustle ¬£500/month': {'other_income': lambda x: x + 6000},\n",
        "        'Rent spare room': {'other_income': lambda x: x + 4800},\n",
        "    }\n",
        "    return analyze_customers(customer_ids, n, scenarios=income_scenarios)\n",
        "\n",
        "def analyze_with_spending_focus(customer_ids=None, n=3):\n",
        "    \"\"\"Analyze focusing on spending reduction only.\"\"\"\n",
        "    spending_scenarios = {\n",
        "        'Stop gambling': {'gambling_spend': 0.0},\n",
        "        'Reduce gambling 50%': {'gambling_spend': 0.5},\n",
        "        'Remortgage (15% housing reduction)': {'housing_spend': 0.85},\n",
        "        'Childcare vouchers': {'childcare_spend': 0.75},\n",
        "        'Cut all discretionary 20%': {'gambling_spend': 0.8, 'housing_spend': 0.95},\n",
        "    }\n",
        "    return analyze_customers(customer_ids, n, scenarios=spending_scenarios)\n",
        "\n",
        "# Usage:\n",
        "analyze_with_income_focus(n=2)  # Customer 24070 with income scenarios\n",
        "analyze_with_spending_focus(n=2)  # First 5 with spending scenarios\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES1RAzghCecE",
        "outputId": "6351aa26-c9b7-4eb9-c0d8-e850e154e579"
      },
      "id": "ES1RAzghCecE",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Analyzing first 2 customers\n",
            "\n",
            "================================================================================\n",
            "CUSTOMER 1/2 - ID: 1 - Savings: ¬£37,712.97\n",
            "================================================================================\n",
            "\n",
            "üîÆ Running Counterfactual Analysis using: Random Forest\n",
            "üìä Baseline prediction: ¬£37,800.34\n",
            "üìä Actual savings: ¬£37,712.97\n",
            "  ‚úì Reduce housing spend by 20%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Reduce housing spend by 50%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Eliminate gambling spend: ¬£37,800.34 (+¬£0.00, +0.0%)\n",
            "  ‚úì Reduce childcare spend by 30%: ¬£37,800.34 (+¬£0.00, +0.0%)\n",
            "  ‚úì Increase salary by 10%: ¬£37,868.16 (+¬£67.82, +0.2%)\n",
            "  ‚úì Increase salary by 20%: ¬£37,868.16 (+¬£67.82, +0.2%)\n",
            "  ‚úì Reduce all discretionary spending by 25%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Increase other income by 50%: ¬£37,825.23 (+¬£24.89, +0.1%)\n",
            "\n",
            "‚úì Report saved: analysis_outputs/counterfactual_analysis_customer_1.md\n",
            "\n",
            "================================================================================\n",
            "AI RECOMMENDATIONS:\n",
            "================================================================================\n",
            "### 1. Most Impactful Actions\n",
            "The scenarios that provide the greatest savings improvement are:\n",
            "\n",
            "1. **Increase Salary by 10%** and **20%**: Both scenarios yield an increase of ¬£67.82 (+0.2%). While the percentage increase in savings is minimal, it is the most impactful action available based on the scenarios provided.\n",
            "\n",
            "2. **Increase Other Income by 50%**: This scenario results in a predicted savings increase of ¬£24.89 (+0.1%). While it is less impactful than the salary increase, it still contributes positively to overall savings.\n",
            "\n",
            "The reason these actions are impactful is that they directly increase the income, which has a more significant effect on the savings prediction compared to reductions in spending that yield no change or negative impacts.\n",
            "\n",
            "### 2. Feasibility Assessment\n",
            "The most realistic changes for this customer to implement are:\n",
            "\n",
            "1. **Increase Salary by 10% or 20%**: If the customer is in a position to negotiate a raise or seek a higher-paying job, this could be feasible, especially given their age and experience.\n",
            "\n",
            "2. **Increase Other Income by 50%**: This could be achievable through side jobs, freelance work, or investments, depending on the customer‚Äôs skills and resources.\n",
            "\n",
            "3. **Eliminate Gambling Spend**: Since the customer currently has no gambling spend, this scenario is not applicable.\n",
            "\n",
            "4. **Reduce Childcare Spend by 30%**: This is also not applicable as there are no childcare expenses.\n",
            "\n",
            "5. **Reduce Housing Spend by 20% or 50%**: This may be challenging as it often involves moving or renegotiating leases, which can be disruptive.\n",
            "\n",
            "### 3. Personalized Recommendations\n",
            "Here are specific, actionable recommendations ranked by expected impact, ease of implementation, and timeline:\n",
            "\n",
            "1. **Increase Salary by 10%**:\n",
            "   - **Expected Impact**: +¬£67.82/year\n",
            "   - **Ease of Implementation**: Moderate (depends on job situation)\n",
            "   - **Timeline**: Short-term (if negotiations are successful)\n",
            "\n",
            "2. **Increase Other Income by 50%**:\n",
            "   - **Expected Impact**: +¬£24.89/year\n",
            "   - **Ease of Implementation**: Moderate (requires effort to find opportunities)\n",
            "   - **Timeline**: Short-term to medium-term (depends on finding suitable work)\n",
            "\n",
            "3. **Reduce Housing Spend by 20%**:\n",
            "   - **Expected Impact**: -¬£73.84/year (not favorable but could be considered)\n",
            "   - **Ease of Implementation**: Difficult (requires significant lifestyle changes)\n",
            "   - **Timeline**: Long-term (if moving is necessary)\n",
            "\n",
            "4. **Eliminate Gambling Spend**: \n",
            "   - **Expected Impact**: ¬£0.00 (not applicable)\n",
            "   - **Ease of Implementation**: N/A\n",
            "   - **Timeline**: N/A\n",
            "\n",
            "5. **Reduce Childcare Spend by 30%**: \n",
            "   - **Expected Impact**: ¬£0.00 (not applicable)\n",
            "   - **Ease of Implementation**: N/A\n",
            "   - **Timeline**: N/A\n",
            "\n",
            "### 4. Combined Strategy\n",
            "To maximize savings while remaining achievable, I recommend the following combination:\n",
            "\n",
            "1. **Increase Salary by 10%**: This is the most straightforward way to increase savings with minimal disruption.\n",
            "2. **Increase Other Income by 50%**: This can be pursued alongside the salary increase, providing an additional boost to savings.\n",
            "3. **Consider Reducing Housing Spend by 20%**: While this is challenging, if the customer is open to relocating or negotiating their current housing situation, it could provide additional savings.\n",
            "\n",
            "By focusing on increasing income through salary negotiation and side work, the customer can enhance their savings without making drastic lifestyle changes.\n",
            "‚úÖ Report saved: analysis_outputs/counterfactual_analysis_customer_1.md\n",
            "\n",
            "================================================================================\n",
            "CUSTOMER 2/2 - ID: 2 - Savings: ¬£27,008.50\n",
            "================================================================================\n",
            "\n",
            "üîÆ Running Counterfactual Analysis using: Random Forest\n",
            "üìä Baseline prediction: ¬£27,019.83\n",
            "üìä Actual savings: ¬£27,008.50\n",
            "  ‚úì Reduce housing spend by 20%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Reduce housing spend by 50%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Eliminate gambling spend: ¬£27,019.83 (+¬£0.00, +0.0%)\n",
            "  ‚úì Reduce childcare spend by 30%: ¬£27,019.83 (+¬£0.00, +0.0%)\n",
            "  ‚úì Increase salary by 10%: ¬£27,118.71 (+¬£98.87, +0.4%)\n",
            "  ‚úì Increase salary by 20%: ¬£27,118.71 (+¬£98.87, +0.4%)\n",
            "  ‚úì Reduce all discretionary spending by 25%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Increase other income by 50%: ¬£27,077.31 (+¬£57.48, +0.2%)\n",
            "\n",
            "‚úì Report saved: analysis_outputs/counterfactual_analysis_customer_2.md\n",
            "\n",
            "================================================================================\n",
            "AI RECOMMENDATIONS:\n",
            "================================================================================\n",
            "### 1. Most Impactful Actions\n",
            "The two scenarios that provide the greatest savings improvement are:\n",
            "\n",
            "- **Increase Salary by 10%**: This scenario results in a predicted savings increase of ¬£98.87 (+0.4%). \n",
            "- **Increase Other Income by 50%**: This scenario leads to a predicted savings increase of ¬£57.48 (+0.2%).\n",
            "\n",
            "These scenarios are impactful because they directly increase the customer's income, which has a more significant effect on savings compared to reductions in spending.\n",
            "\n",
            "### 2. Feasibility Assessment\n",
            "The most realistic changes for this customer to implement are:\n",
            "\n",
            "- **Increase Salary by 10% or 20%**: Depending on the customer's current job situation, negotiating a raise or seeking a higher-paying position could be feasible, especially given the customer's age and experience.\n",
            "- **Increase Other Income by 50%**: This could be achieved through part-time work, freelance opportunities, or investments, which might be more manageable than drastic changes in spending.\n",
            "\n",
            "Eliminating gambling spend or reducing childcare spend may not be applicable since the customer has no current gambling or childcare expenses.\n",
            "\n",
            "### 3. Personalized Recommendations\n",
            "Here are 3-5 specific, actionable recommendations ranked by expected impact on savings, ease of implementation, and timeline:\n",
            "\n",
            "1. **Increase Salary by 10%**\n",
            "   - **Expected Impact**: +¬£98.87/year\n",
            "   - **Ease of Implementation**: Moderate (requires negotiation or job change)\n",
            "   - **Timeline**: Short-term (within the next year)\n",
            "\n",
            "2. **Increase Other Income by 50%**\n",
            "   - **Expected Impact**: +¬£57.48/year\n",
            "   - **Ease of Implementation**: Moderate (may require finding freelance work or side jobs)\n",
            "   - **Timeline**: Short-term to medium-term (within the next 6-12 months)\n",
            "\n",
            "3. **Eliminate Gambling Spend**\n",
            "   - **Expected Impact**: ¬£0.00 (already at ¬£0)\n",
            "   - **Ease of Implementation**: High (no action needed)\n",
            "   - **Timeline**: Immediate\n",
            "\n",
            "4. **Reduce Housing Spend by 20%**\n",
            "   - **Expected Impact**: -¬£21.99/year (not beneficial)\n",
            "   - **Ease of Implementation**: Moderate (may require moving or refinancing)\n",
            "   - **Timeline**: Long-term (could take several months)\n",
            "\n",
            "5. **Reduce All Discretionary Spending by 25%**\n",
            "   - **Expected Impact**: -¬£21.99/year (not beneficial)\n",
            "   - **Ease of Implementation**: Moderate (requires lifestyle changes)\n",
            "   - **Timeline**: Long-term (ongoing)\n",
            "\n",
            "### 4. Combined Strategy\n",
            "To maximize savings while remaining achievable, I recommend the following combination:\n",
            "\n",
            "1. **Increase Salary by 10%**: This is a direct way to boost income and savings.\n",
            "2. **Increase Other Income by 50%**: Explore freelance opportunities or part-time work to supplement income.\n",
            "3. **Eliminate Gambling Spend**: While the customer already spends ¬£0 on gambling, maintaining this discipline is crucial.\n",
            "\n",
            "This combined strategy focuses on increasing income rather than reducing spending, which is more sustainable and likely to yield better long-term results. The expected total increase in savings from the first two actions would be approximately ¬£156.35/year, which is a significant improvement compared to the current savings level.\n",
            "‚úÖ Report saved: analysis_outputs/counterfactual_analysis_customer_2.md\n",
            "üìã Analyzing first 2 customers\n",
            "\n",
            "================================================================================\n",
            "CUSTOMER 1/2 - ID: 1 - Savings: ¬£37,712.97\n",
            "================================================================================\n",
            "\n",
            "üîÆ Running Counterfactual Analysis using: Random Forest\n",
            "üìä Baseline prediction: ¬£37,800.34\n",
            "üìä Actual savings: ¬£37,712.97\n",
            "  ‚úì Reduce housing spend by 20%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Reduce housing spend by 50%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Eliminate gambling spend: ¬£37,800.34 (+¬£0.00, +0.0%)\n",
            "  ‚úì Reduce childcare spend by 30%: ¬£37,800.34 (+¬£0.00, +0.0%)\n",
            "  ‚úì Increase salary by 10%: ¬£37,868.16 (+¬£67.82, +0.2%)\n",
            "  ‚úì Increase salary by 20%: ¬£37,868.16 (+¬£67.82, +0.2%)\n",
            "  ‚úì Reduce all discretionary spending by 25%: ¬£37,726.51 (¬£-73.84, -0.2%)\n",
            "  ‚úì Increase other income by 50%: ¬£37,825.23 (+¬£24.89, +0.1%)\n",
            "\n",
            "‚úì Report saved: analysis_outputs/counterfactual_analysis_customer_1.md\n",
            "\n",
            "================================================================================\n",
            "AI RECOMMENDATIONS:\n",
            "================================================================================\n",
            "### 1. Most Impactful Actions\n",
            "The two scenarios that provide the greatest savings improvement, based on the predicted changes, are:\n",
            "\n",
            "- **Increase Salary by 20%**: This scenario results in a predicted savings increase of ¬£67.82 (+0.2%). While the percentage increase is small, it is the highest among the scenarios analyzed.\n",
            "  \n",
            "- **Increase Other Income by 50%**: This scenario results in a predicted savings increase of ¬£24.89 (+0.1%). Although the increase is lower than the salary increase, it still contributes positively to overall savings.\n",
            "\n",
            "These scenarios are impactful because they directly increase the customer's income, which has a more significant effect on savings compared to reducing expenses.\n",
            "\n",
            "### 2. Feasibility Assessment\n",
            "The most realistic changes for this customer to implement are:\n",
            "\n",
            "- **Increase Salary by 10% or 20%**: Depending on the customer's current job situation, negotiating a raise or seeking a higher-paying position could be feasible, especially given their age and experience.\n",
            "  \n",
            "- **Increase Other Income by 50%**: This could be achieved through side jobs, freelance work, or investments. Given the customer's current other income level, this may require some effort but is achievable.\n",
            "\n",
            "The other scenarios, such as eliminating gambling spend or reducing childcare spend, are not impactful since they do not change the predicted savings. Reducing housing spend may not be realistic given the current market conditions.\n",
            "\n",
            "### 3. Personalized Recommendations\n",
            "Here are 3-5 specific, actionable recommendations ranked by expected impact on savings, ease of implementation, and timeline:\n",
            "\n",
            "1. **Negotiate a Salary Increase (10-20%)**:\n",
            "   - **Expected Impact**: +¬£67.82 (20% increase)\n",
            "   - **Ease of Implementation**: Moderate (requires negotiation skills)\n",
            "   - **Timeline**: Short-term (within the next year)\n",
            "\n",
            "2. **Explore Additional Income Opportunities (50% increase in other income)**:\n",
            "   - **Expected Impact**: +¬£24.89\n",
            "   - **Ease of Implementation**: Moderate (may require time investment)\n",
            "   - **Timeline**: Short-term to medium-term (3-6 months)\n",
            "\n",
            "3. **Review and Optimize Housing Expenses**:\n",
            "   - **Expected Impact**: Minimal, but necessary for overall financial health.\n",
            "   - **Ease of Implementation**: Moderate (requires research and potential relocation)\n",
            "   - **Timeline**: Long-term (6 months to 1 year)\n",
            "\n",
            "4. **Create a Budget to Track Discretionary Spending**:\n",
            "   - **Expected Impact**: Indirectly improves savings by making the customer aware of spending habits.\n",
            "   - **Ease of Implementation**: Easy (requires commitment to track spending)\n",
            "   - **Timeline**: Short-term (immediate)\n",
            "\n",
            "5. **Consider Financial Counseling or Workshops**:\n",
            "   - **Expected Impact**: Can lead to better financial decisions and savings strategies.\n",
            "   - **Ease of Implementation**: Easy (many free resources available)\n",
            "   - **Timeline**: Short-term (immediate)\n",
            "\n",
            "### 4. Combined Strategy\n",
            "To maximize savings while remaining achievable, I recommend the following combination of changes:\n",
            "\n",
            "1. **Negotiate a Salary Increase (10-20%)**: This is the most impactful change that can be implemented relatively quickly.\n",
            "  \n",
            "2. **Explore Additional Income Opportunities**: This can supplement the salary increase and provide a buffer for unexpected expenses.\n",
            "\n",
            "3. **Create a Budget to Track Discretionary Spending**: This will help the customer become more aware of their spending habits and identify areas for potential savings.\n",
            "\n",
            "By focusing on these three strategies, the customer can effectively increase their savings while maintaining a realistic approach to implementation.\n",
            "‚úÖ Report saved: analysis_outputs/counterfactual_analysis_customer_1.md\n",
            "\n",
            "================================================================================\n",
            "CUSTOMER 2/2 - ID: 2 - Savings: ¬£27,008.50\n",
            "================================================================================\n",
            "\n",
            "üîÆ Running Counterfactual Analysis using: Random Forest\n",
            "üìä Baseline prediction: ¬£27,019.83\n",
            "üìä Actual savings: ¬£27,008.50\n",
            "  ‚úì Reduce housing spend by 20%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Reduce housing spend by 50%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Eliminate gambling spend: ¬£27,019.83 (+¬£0.00, +0.0%)\n",
            "  ‚úì Reduce childcare spend by 30%: ¬£27,019.83 (+¬£0.00, +0.0%)\n",
            "  ‚úì Increase salary by 10%: ¬£27,118.71 (+¬£98.87, +0.4%)\n",
            "  ‚úì Increase salary by 20%: ¬£27,118.71 (+¬£98.87, +0.4%)\n",
            "  ‚úì Reduce all discretionary spending by 25%: ¬£26,997.84 (¬£-21.99, -0.1%)\n",
            "  ‚úì Increase other income by 50%: ¬£27,077.31 (+¬£57.48, +0.2%)\n",
            "\n",
            "‚úì Report saved: analysis_outputs/counterfactual_analysis_customer_2.md\n",
            "\n",
            "================================================================================\n",
            "AI RECOMMENDATIONS:\n",
            "================================================================================\n",
            "### 1. Most Impactful Actions\n",
            "The two scenarios that provide the greatest savings improvement are:\n",
            "\n",
            "1. **Increase salary by 10%**: This scenario results in a predicted savings increase of ¬£98.87 (+0.4%). Given that salary is a significant portion of income, even a modest increase can lead to a noticeable improvement in savings.\n",
            "\n",
            "2. **Increase other income by 50%**: This scenario predicts an increase of ¬£57.48 (+0.2%). While the increase is smaller than the salary increase, it still represents a meaningful boost to overall income.\n",
            "\n",
            "Both scenarios are impactful because they directly increase the income, which has a more substantial effect on savings compared to reducing expenses.\n",
            "\n",
            "### 2. Feasibility Assessment\n",
            "The most realistic changes for this customer to implement are:\n",
            "\n",
            "1. **Increase other income by 50%**: This could be feasible through side jobs, freelance work, or investments. The customer may already have skills or resources that can be leveraged for additional income.\n",
            "\n",
            "2. **Eliminate gambling spend**: Since the customer currently spends ¬£0 on gambling, this scenario is already implemented and thus has no impact.\n",
            "\n",
            "3. **Reduce childcare spend by 30%**: This is not applicable as the customer has no childcare expenses.\n",
            "\n",
            "The least feasible changes are the salary increases, as they typically require negotiation or a change in employment, which may not be immediate.\n",
            "\n",
            "### 3. Personalized Recommendations\n",
            "Here are 3-5 specific, actionable recommendations ranked by expected impact, ease of implementation, and timeline:\n",
            "\n",
            "1. **Increase Other Income by 50%**:\n",
            "   - **Expected Impact**: +¬£57.48/year\n",
            "   - **Ease of Implementation**: Moderate (requires effort to find opportunities)\n",
            "   - **Timeline**: Short-term (can start immediately)\n",
            "\n",
            "2. **Negotiate a Salary Increase**:\n",
            "   - **Expected Impact**: +¬£98.87/year (10% increase)\n",
            "   - **Ease of Implementation**: Moderate to difficult (depends on employer)\n",
            "   - **Timeline**: Short-term to medium-term (depends on negotiation process)\n",
            "\n",
            "3. **Review and Optimize Housing Spend**:\n",
            "   - **Expected Impact**: While reducing housing spend by 50% results in a negative change, reviewing current housing costs for potential savings (e.g., refinancing, moving to a less expensive area) could yield positive results.\n",
            "   - **Ease of Implementation**: Moderate (may require research and decision-making)\n",
            "   - **Timeline**: Medium-term (depends on market conditions)\n",
            "\n",
            "4. **Eliminate Discretionary Spending**:\n",
            "   - **Expected Impact**: While the model shows no change, reducing discretionary spending can free up funds for savings.\n",
            "   - **Ease of Implementation**: Easy (requires self-discipline)\n",
            "   - **Timeline**: Short-term (can start immediately)\n",
            "\n",
            "5. **Create a Budget Plan**:\n",
            "   - **Expected Impact**: Indirect but can lead to better savings management.\n",
            "   - **Ease of Implementation**: Easy (requires time to set up)\n",
            "   - **Timeline**: Short-term (can start immediately)\n",
            "\n",
            "### 4. Combined Strategy\n",
            "To maximize savings while remaining achievable, I recommend the following combination of changes:\n",
            "\n",
            "1. **Increase Other Income by 50%**: Actively seek additional income sources through freelance work or side jobs. This is the most straightforward way to increase savings.\n",
            "\n",
            "2. **Negotiate a Salary Increase**: Prepare for a salary negotiation with your employer. Research market rates for your position and gather evidence of your contributions to justify the request.\n",
            "\n",
            "3. **Create a Budget Plan**: Establish a budget to track spending and identify areas where discretionary spending can be reduced. This will help in managing finances better and ensuring that any additional income goes directly into savings.\n",
            "\n",
            "By focusing on these three strategies, the customer can potentially increase their savings significantly while implementing changes that are realistic and manageable.\n",
            "‚úÖ Report saved: analysis_outputs/counterfactual_analysis_customer_2.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171ff4a7"
      },
      "source": [
        "#### **Comment**\n",
        "Large language models generally underperform when used to process structured/table data. Instead, I have tapped into it's strength as a text generator to create a scenario based counterfactual analysis. The scenarios explored in the counterfactual analysis, while illustrative and \"made up\" for the purpose of demonstrating the concept, are valuable for exploring potential impacts on savings. By creating these hypothetical situations, we can gain insights into which factors are most sensitive to change and use an LLM to generate personalized and human readable recommendations. This approach allows us to provide customers with concrete examples of how changes in their financial behavior or circumstances could affect their savings goals."
      ],
      "id": "171ff4a7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Comment**\n"
      ],
      "metadata": {
        "id": "UKmcbTIoFymM"
      },
      "id": "UKmcbTIoFymM"
    },
    {
      "cell_type": "markdown",
      "id": "5d6ce8f6",
      "metadata": {
        "id": "5d6ce8f6"
      },
      "source": [
        "## **Conclusion**\n",
        "This assignment evaluates technical competencies in coding, data analytics and operationalising ethics; however, these capabilities must be understood within their broader business context.\n",
        "Effective ethical ML implementation requires clear alignment with defined business goals to demonstrate tangible value and practical applicability. Drawing from Lloyds Banking Group's 2025-26 strategic priorities, the following business objectives are relevant contexts for this savings prediction exercise:\n",
        "\n",
        "1. Deepening customer relationships through behavioral insights ‚Äì Predictive models can identify high-propensity savers and inform targeted engagement strategies to increase depth of relationship (current: c.1% growth in H1 2025; 2026 target: c.3%) [page 6].\n",
        "2. Optimizing deposit franchise economics ‚Äì Understanding drivers of savings behavior supports pricing strategies and customer segmentation to improve deposit gross margins (H1 2025: 1.29%, up 16bps YoY) [page 16].\n",
        "3. Supporting sustainable financing commitments ‚Äì Identifying customers with capacity and propensity to save can inform targeting for green savings products aligned with the ¬£30bn sustainable financing target by 2026 [page 6].\n",
        "4. Enhancing capital-lite revenue growth ‚Äì Predictive insights into savings patterns can support fee-based product cross-sell, contributing to the 50:50 NII:OOI revenue split target and >¬£1.5bn additional strategic revenues by 2026 [page 7].\n",
        "5. Driving operational efficiency through automation ‚Äì ML models can automate customer segmentation and propensity scoring, supporting the <50% cost-to-income ratio target and productivity improvements (>40% increase in customers served per FTE vs. 2021) [page 7,9,19].\n",
        "6. Risk-aware portfolio management ‚Äì Understanding the financial health indicators within savings behavior supports prudent lending decisions and maintaining robust asset quality (target AQR: c.25bps) [page 20]."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t07grXwTBEIC"
      },
      "id": "t07grXwTBEIC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}